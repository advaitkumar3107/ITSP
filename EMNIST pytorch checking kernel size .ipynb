{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. Loss: 3.2990, Accuracy: 796/20800 (3%)\n",
      "\n",
      "Train Epoch: 1 [0/124800 (0%)]\tLoss: 3.312088\n",
      "Train Epoch: 1 [2500/124800 (2%)]\tLoss: 3.304320\n",
      "Train Epoch: 1 [5000/124800 (4%)]\tLoss: 3.259950\n",
      "Train Epoch: 1 [7500/124800 (6%)]\tLoss: 3.264279\n",
      "Train Epoch: 1 [10000/124800 (8%)]\tLoss: 3.238531\n",
      "Train Epoch: 1 [12500/124800 (10%)]\tLoss: 3.224043\n",
      "Train Epoch: 1 [15000/124800 (12%)]\tLoss: 3.048221\n",
      "Train Epoch: 1 [17500/124800 (14%)]\tLoss: 2.623239\n",
      "Train Epoch: 1 [20000/124800 (16%)]\tLoss: 2.226000\n",
      "Train Epoch: 1 [22500/124800 (18%)]\tLoss: 1.857546\n",
      "Train Epoch: 1 [25000/124800 (20%)]\tLoss: 1.768518\n",
      "Train Epoch: 1 [27500/124800 (22%)]\tLoss: 1.326069\n",
      "Train Epoch: 1 [30000/124800 (24%)]\tLoss: 1.533984\n",
      "Train Epoch: 1 [32500/124800 (26%)]\tLoss: 1.419057\n",
      "Train Epoch: 1 [35000/124800 (28%)]\tLoss: 1.107064\n",
      "Train Epoch: 1 [37500/124800 (30%)]\tLoss: 1.031472\n",
      "Train Epoch: 1 [40000/124800 (32%)]\tLoss: 1.156525\n",
      "Train Epoch: 1 [42500/124800 (34%)]\tLoss: 0.982761\n",
      "Train Epoch: 1 [45000/124800 (36%)]\tLoss: 0.871397\n",
      "Train Epoch: 1 [47500/124800 (38%)]\tLoss: 0.976269\n",
      "Train Epoch: 1 [50000/124800 (40%)]\tLoss: 0.742063\n",
      "Train Epoch: 1 [52500/124800 (42%)]\tLoss: 0.568260\n",
      "Train Epoch: 1 [55000/124800 (44%)]\tLoss: 0.730615\n",
      "Train Epoch: 1 [57500/124800 (46%)]\tLoss: 0.696037\n",
      "Train Epoch: 1 [60000/124800 (48%)]\tLoss: 0.522809\n",
      "Train Epoch: 1 [62500/124800 (50%)]\tLoss: 0.797537\n",
      "Train Epoch: 1 [65000/124800 (52%)]\tLoss: 0.462512\n",
      "Train Epoch: 1 [67500/124800 (54%)]\tLoss: 0.762556\n",
      "Train Epoch: 1 [70000/124800 (56%)]\tLoss: 1.049979\n",
      "Train Epoch: 1 [72500/124800 (58%)]\tLoss: 0.652929\n",
      "Train Epoch: 1 [75000/124800 (60%)]\tLoss: 0.498984\n",
      "Train Epoch: 1 [77500/124800 (62%)]\tLoss: 0.760545\n",
      "Train Epoch: 1 [80000/124800 (64%)]\tLoss: 0.275649\n",
      "Train Epoch: 1 [82500/124800 (66%)]\tLoss: 0.715452\n",
      "Train Epoch: 1 [85000/124800 (68%)]\tLoss: 0.505501\n",
      "Train Epoch: 1 [87500/124800 (70%)]\tLoss: 0.493101\n",
      "Train Epoch: 1 [90000/124800 (72%)]\tLoss: 0.949379\n",
      "Train Epoch: 1 [92500/124800 (74%)]\tLoss: 0.653839\n",
      "Train Epoch: 1 [95000/124800 (76%)]\tLoss: 0.486745\n",
      "Train Epoch: 1 [97500/124800 (78%)]\tLoss: 0.664664\n",
      "Train Epoch: 1 [100000/124800 (80%)]\tLoss: 0.691018\n",
      "Train Epoch: 1 [102500/124800 (82%)]\tLoss: 0.399536\n",
      "Train Epoch: 1 [105000/124800 (84%)]\tLoss: 0.169872\n",
      "Train Epoch: 1 [107500/124800 (86%)]\tLoss: 0.423938\n",
      "Train Epoch: 1 [110000/124800 (88%)]\tLoss: 0.472768\n",
      "Train Epoch: 1 [112500/124800 (90%)]\tLoss: 0.532549\n",
      "Train Epoch: 1 [115000/124800 (92%)]\tLoss: 0.437097\n",
      "Train Epoch: 1 [117500/124800 (94%)]\tLoss: 0.537223\n",
      "Train Epoch: 1 [120000/124800 (96%)]\tLoss: 0.405635\n",
      "Train Epoch: 1 [122500/124800 (98%)]\tLoss: 0.515411\n",
      "\n",
      "Test set: Avg. Loss: 0.4047, Accuracy: 18031/20800 (86%)\n",
      "\n",
      "Train Epoch: 2 [0/124800 (0%)]\tLoss: 0.343527\n",
      "Train Epoch: 2 [2500/124800 (2%)]\tLoss: 0.273484\n",
      "Train Epoch: 2 [5000/124800 (4%)]\tLoss: 0.623325\n",
      "Train Epoch: 2 [7500/124800 (6%)]\tLoss: 0.257284\n",
      "Train Epoch: 2 [10000/124800 (8%)]\tLoss: 0.387110\n",
      "Train Epoch: 2 [12500/124800 (10%)]\tLoss: 0.392848\n",
      "Train Epoch: 2 [15000/124800 (12%)]\tLoss: 0.366385\n",
      "Train Epoch: 2 [17500/124800 (14%)]\tLoss: 0.390901\n",
      "Train Epoch: 2 [20000/124800 (16%)]\tLoss: 0.374874\n",
      "Train Epoch: 2 [22500/124800 (18%)]\tLoss: 0.440701\n",
      "Train Epoch: 2 [25000/124800 (20%)]\tLoss: 0.329817\n",
      "Train Epoch: 2 [27500/124800 (22%)]\tLoss: 0.296159\n",
      "Train Epoch: 2 [30000/124800 (24%)]\tLoss: 0.520236\n",
      "Train Epoch: 2 [32500/124800 (26%)]\tLoss: 0.484831\n",
      "Train Epoch: 2 [35000/124800 (28%)]\tLoss: 0.565646\n",
      "Train Epoch: 2 [37500/124800 (30%)]\tLoss: 0.319799\n",
      "Train Epoch: 2 [40000/124800 (32%)]\tLoss: 0.309669\n",
      "Train Epoch: 2 [42500/124800 (34%)]\tLoss: 0.449344\n",
      "Train Epoch: 2 [45000/124800 (36%)]\tLoss: 0.357830\n",
      "Train Epoch: 2 [47500/124800 (38%)]\tLoss: 0.339591\n",
      "Train Epoch: 2 [50000/124800 (40%)]\tLoss: 0.285539\n",
      "Train Epoch: 2 [52500/124800 (42%)]\tLoss: 0.374110\n",
      "Train Epoch: 2 [55000/124800 (44%)]\tLoss: 0.305903\n",
      "Train Epoch: 2 [57500/124800 (46%)]\tLoss: 0.346951\n",
      "Train Epoch: 2 [60000/124800 (48%)]\tLoss: 0.459336\n",
      "Train Epoch: 2 [62500/124800 (50%)]\tLoss: 0.353156\n",
      "Train Epoch: 2 [65000/124800 (52%)]\tLoss: 0.226044\n",
      "Train Epoch: 2 [67500/124800 (54%)]\tLoss: 0.463918\n",
      "Train Epoch: 2 [70000/124800 (56%)]\tLoss: 0.337141\n",
      "Train Epoch: 2 [72500/124800 (58%)]\tLoss: 0.379077\n",
      "Train Epoch: 2 [75000/124800 (60%)]\tLoss: 0.255815\n",
      "Train Epoch: 2 [77500/124800 (62%)]\tLoss: 0.385674\n",
      "Train Epoch: 2 [80000/124800 (64%)]\tLoss: 0.221530\n",
      "Train Epoch: 2 [82500/124800 (66%)]\tLoss: 0.330923\n",
      "Train Epoch: 2 [85000/124800 (68%)]\tLoss: 0.438192\n",
      "Train Epoch: 2 [87500/124800 (70%)]\tLoss: 0.344058\n",
      "Train Epoch: 2 [90000/124800 (72%)]\tLoss: 0.557735\n",
      "Train Epoch: 2 [92500/124800 (74%)]\tLoss: 0.410817\n",
      "Train Epoch: 2 [95000/124800 (76%)]\tLoss: 0.392361\n",
      "Train Epoch: 2 [97500/124800 (78%)]\tLoss: 0.230076\n",
      "Train Epoch: 2 [100000/124800 (80%)]\tLoss: 0.220003\n",
      "Train Epoch: 2 [102500/124800 (82%)]\tLoss: 0.345328\n",
      "Train Epoch: 2 [105000/124800 (84%)]\tLoss: 0.414970\n",
      "Train Epoch: 2 [107500/124800 (86%)]\tLoss: 0.248659\n",
      "Train Epoch: 2 [110000/124800 (88%)]\tLoss: 0.130253\n",
      "Train Epoch: 2 [112500/124800 (90%)]\tLoss: 0.425672\n",
      "Train Epoch: 2 [115000/124800 (92%)]\tLoss: 0.375629\n",
      "Train Epoch: 2 [117500/124800 (94%)]\tLoss: 0.334573\n",
      "Train Epoch: 2 [120000/124800 (96%)]\tLoss: 0.279663\n",
      "Train Epoch: 2 [122500/124800 (98%)]\tLoss: 0.275812\n",
      "\n",
      "Test set: Avg. Loss: 0.3240, Accuracy: 18598/20800 (89%)\n",
      "\n",
      "Train Epoch: 3 [0/124800 (0%)]\tLoss: 0.372576\n",
      "Train Epoch: 3 [2500/124800 (2%)]\tLoss: 0.245279\n",
      "Train Epoch: 3 [5000/124800 (4%)]\tLoss: 0.279017\n",
      "Train Epoch: 3 [7500/124800 (6%)]\tLoss: 0.436382\n",
      "Train Epoch: 3 [10000/124800 (8%)]\tLoss: 0.223031\n",
      "Train Epoch: 3 [12500/124800 (10%)]\tLoss: 0.229350\n",
      "Train Epoch: 3 [15000/124800 (12%)]\tLoss: 0.172552\n",
      "Train Epoch: 3 [17500/124800 (14%)]\tLoss: 0.259795\n",
      "Train Epoch: 3 [20000/124800 (16%)]\tLoss: 0.343428\n",
      "Train Epoch: 3 [22500/124800 (18%)]\tLoss: 0.199040\n",
      "Train Epoch: 3 [25000/124800 (20%)]\tLoss: 0.281241\n",
      "Train Epoch: 3 [27500/124800 (22%)]\tLoss: 0.417172\n",
      "Train Epoch: 3 [30000/124800 (24%)]\tLoss: 0.231641\n",
      "Train Epoch: 3 [32500/124800 (26%)]\tLoss: 0.324735\n",
      "Train Epoch: 3 [35000/124800 (28%)]\tLoss: 0.386864\n",
      "Train Epoch: 3 [37500/124800 (30%)]\tLoss: 0.386256\n",
      "Train Epoch: 3 [40000/124800 (32%)]\tLoss: 0.332889\n",
      "Train Epoch: 3 [42500/124800 (34%)]\tLoss: 0.203334\n",
      "Train Epoch: 3 [45000/124800 (36%)]\tLoss: 0.146060\n",
      "Train Epoch: 3 [47500/124800 (38%)]\tLoss: 0.382110\n",
      "Train Epoch: 3 [50000/124800 (40%)]\tLoss: 0.508868\n",
      "Train Epoch: 3 [52500/124800 (42%)]\tLoss: 0.371251\n",
      "Train Epoch: 3 [55000/124800 (44%)]\tLoss: 0.226949\n",
      "Train Epoch: 3 [57500/124800 (46%)]\tLoss: 0.229185\n",
      "Train Epoch: 3 [60000/124800 (48%)]\tLoss: 0.286757\n",
      "Train Epoch: 3 [62500/124800 (50%)]\tLoss: 0.293213\n",
      "Train Epoch: 3 [65000/124800 (52%)]\tLoss: 0.206565\n",
      "Train Epoch: 3 [67500/124800 (54%)]\tLoss: 0.186465\n",
      "Train Epoch: 3 [70000/124800 (56%)]\tLoss: 0.347481\n",
      "Train Epoch: 3 [72500/124800 (58%)]\tLoss: 0.428067\n",
      "Train Epoch: 3 [75000/124800 (60%)]\tLoss: 0.233065\n",
      "Train Epoch: 3 [77500/124800 (62%)]\tLoss: 0.319005\n",
      "Train Epoch: 3 [80000/124800 (64%)]\tLoss: 0.313942\n",
      "Train Epoch: 3 [82500/124800 (66%)]\tLoss: 0.121376\n",
      "Train Epoch: 3 [85000/124800 (68%)]\tLoss: 0.152924\n",
      "Train Epoch: 3 [87500/124800 (70%)]\tLoss: 0.211164\n",
      "Train Epoch: 3 [90000/124800 (72%)]\tLoss: 0.323073\n",
      "Train Epoch: 3 [92500/124800 (74%)]\tLoss: 0.503040\n",
      "Train Epoch: 3 [95000/124800 (76%)]\tLoss: 0.453533\n",
      "Train Epoch: 3 [97500/124800 (78%)]\tLoss: 0.212492\n",
      "Train Epoch: 3 [100000/124800 (80%)]\tLoss: 0.154426\n",
      "Train Epoch: 3 [102500/124800 (82%)]\tLoss: 0.273396\n",
      "Train Epoch: 3 [105000/124800 (84%)]\tLoss: 0.313920\n",
      "Train Epoch: 3 [107500/124800 (86%)]\tLoss: 0.239521\n",
      "Train Epoch: 3 [110000/124800 (88%)]\tLoss: 0.187326\n",
      "Train Epoch: 3 [112500/124800 (90%)]\tLoss: 0.208818\n",
      "Train Epoch: 3 [115000/124800 (92%)]\tLoss: 0.173908\n",
      "Train Epoch: 3 [117500/124800 (94%)]\tLoss: 0.222691\n",
      "Train Epoch: 3 [120000/124800 (96%)]\tLoss: 0.122143\n",
      "Train Epoch: 3 [122500/124800 (98%)]\tLoss: 0.310249\n",
      "\n",
      "Test set: Avg. Loss: 0.2912, Accuracy: 18797/20800 (90%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importing the training data from the training file already present in EMNIST dataset\n",
    "training = torch.utils.data.DataLoader(torchvision.datasets.EMNIST(root = '/results/', split = 'letters' , train = True, download = True, transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),torchvision.transforms.Normalize((0.1307,),(0.3081,))])),batch_size = 50, shuffle = True)\n",
    "    # importing the testing data\n",
    "testing = torch.utils.data.DataLoader(torchvision.datasets.EMNIST(root = '/results/', split = 'letters' , train = False, download = True, transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),torchvision.transforms.Normalize((0.1307,),(0.3081,))])),batch_size = 1000, shuffle = True)   \n",
    "    # creating the neural network class. this class will consist of 3 convolutional layers, 3 max pooling layers and 2 linear layers for now. these layers will be connected by the widely used ReLU activation function. we will be using the Log softmax function for the output layer. these parameters will be varied during the subsequent runs to improve the accuracy\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "    #         input is 1X28X28, output is 26 letters\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=3, stride=1, padding=2)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size = 3, stride = 1, padding = 2)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n",
    "        self.conv3 = nn.Conv2d(20, 30, kernel_size = 4, stride = 1, padding = 2)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n",
    "        self.fc1 = nn.Linear(30 * 4 * 4, 150)\n",
    "        self.fc2 = nn.Linear(150, 27)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = x.view(-1, 30 * 4 *4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "            # defining the relationship between input and output sizes for usage afterwards\n",
    "def outputsize(in_size,kernel_size,stride,padding):\n",
    "    output = int((in_size-kernel_size+2*(padding)) / stride) + 1\n",
    "    return output\n",
    "    # defining the various hyperparameters\n",
    "epochs = 3 \n",
    "log_interval = 50\n",
    "network = Net()\n",
    "optimizer = optim.SGD(network.parameters(), lr = 0.01, momentum = 0.21)\n",
    "    # creating counters for our results i.e for training losses, test losses, no. of sample trained and no.of samples tested\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(training.dataset) for i in range(epochs+1)]\n",
    "        # defining the training network\n",
    "def train(epoch):\n",
    "    network.train()\n",
    "    for batch_idx, (data,target) in enumerate(training):\n",
    "    #       making all the gradients zero before the start of each epoch\n",
    "        optimizer.zero_grad()\n",
    "    #     input output and backpropagation steps along with the application of NLL loss function for now\n",
    "        output = network(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    #         printing the results after each mini batch has been trained during a particular epoch\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx*len(data), len(training.dataset), 100. *batch_idx / len(training), loss.item()))\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append((batch_idx*100) + ((epoch-1)*len(training.dataset)))\n",
    "            torch.save(network.state_dict(), '/results/model.pth')\n",
    "            torch.save(optimizer.state_dict(), '/results/optimizer.pth')\n",
    "    # defining the testing network\n",
    "def test():\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    losstest = []\n",
    "    #   calculating the loss for each sample and the accuracy\n",
    "    with torch.no_grad():\n",
    "        for data, target in testing:\n",
    "            output = network(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average = False).item()\n",
    "            pred = output.data.max(1, keepdim = True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    #         initialising the various parameters to calculate accuracy and print it\n",
    "        test_loss /= len(testing.dataset)\n",
    "        losstest.append(test_loss)\n",
    "        print('\\nTest set: Avg. Loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(testing.dataset), 100. *correct / len(testing.dataset)))\n",
    "    # initializing the entire network with the datasets and using the above functions to calculate the accuracy\n",
    "    # first checking the accuracy with random guesses(network uninitialised)\n",
    "test()\n",
    "for epoch in range(1,epochs+1):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. Loss: 3.2986, Accuracy: 775/20800 (3%)\n",
      "\n",
      "Train Epoch: 1 [0/124800 (0%)]\tLoss: 3.307807\n",
      "Train Epoch: 1 [2500/124800 (2%)]\tLoss: 3.279421\n",
      "Train Epoch: 1 [5000/124800 (4%)]\tLoss: 3.254948\n",
      "Train Epoch: 1 [7500/124800 (6%)]\tLoss: 3.192298\n",
      "Train Epoch: 1 [10000/124800 (8%)]\tLoss: 3.069177\n",
      "Train Epoch: 1 [12500/124800 (10%)]\tLoss: 2.214749\n",
      "Train Epoch: 1 [15000/124800 (12%)]\tLoss: 1.872674\n",
      "Train Epoch: 1 [17500/124800 (14%)]\tLoss: 2.109681\n",
      "Train Epoch: 1 [20000/124800 (16%)]\tLoss: 1.762794\n",
      "Train Epoch: 1 [22500/124800 (18%)]\tLoss: 1.466552\n",
      "Train Epoch: 1 [25000/124800 (20%)]\tLoss: 1.275283\n",
      "Train Epoch: 1 [27500/124800 (22%)]\tLoss: 1.312887\n",
      "Train Epoch: 1 [30000/124800 (24%)]\tLoss: 0.918740\n",
      "Train Epoch: 1 [32500/124800 (26%)]\tLoss: 1.052560\n",
      "Train Epoch: 1 [35000/124800 (28%)]\tLoss: 0.951508\n",
      "Train Epoch: 1 [37500/124800 (30%)]\tLoss: 0.879073\n",
      "Train Epoch: 1 [40000/124800 (32%)]\tLoss: 0.970002\n",
      "Train Epoch: 1 [42500/124800 (34%)]\tLoss: 0.617345\n",
      "Train Epoch: 1 [45000/124800 (36%)]\tLoss: 0.889148\n",
      "Train Epoch: 1 [47500/124800 (38%)]\tLoss: 0.430394\n",
      "Train Epoch: 1 [50000/124800 (40%)]\tLoss: 0.484296\n",
      "Train Epoch: 1 [52500/124800 (42%)]\tLoss: 0.831949\n",
      "Train Epoch: 1 [55000/124800 (44%)]\tLoss: 0.559383\n",
      "Train Epoch: 1 [57500/124800 (46%)]\tLoss: 0.569246\n",
      "Train Epoch: 1 [60000/124800 (48%)]\tLoss: 0.671119\n",
      "Train Epoch: 1 [62500/124800 (50%)]\tLoss: 0.515164\n",
      "Train Epoch: 1 [65000/124800 (52%)]\tLoss: 0.393491\n",
      "Train Epoch: 1 [67500/124800 (54%)]\tLoss: 0.506120\n",
      "Train Epoch: 1 [70000/124800 (56%)]\tLoss: 0.566872\n",
      "Train Epoch: 1 [72500/124800 (58%)]\tLoss: 0.652774\n",
      "Train Epoch: 1 [75000/124800 (60%)]\tLoss: 0.448000\n",
      "Train Epoch: 1 [77500/124800 (62%)]\tLoss: 0.448175\n",
      "Train Epoch: 1 [80000/124800 (64%)]\tLoss: 0.545846\n",
      "Train Epoch: 1 [82500/124800 (66%)]\tLoss: 0.267761\n",
      "Train Epoch: 1 [85000/124800 (68%)]\tLoss: 0.277657\n",
      "Train Epoch: 1 [87500/124800 (70%)]\tLoss: 0.241730\n",
      "Train Epoch: 1 [90000/124800 (72%)]\tLoss: 0.590371\n",
      "Train Epoch: 1 [92500/124800 (74%)]\tLoss: 0.438705\n",
      "Train Epoch: 1 [95000/124800 (76%)]\tLoss: 0.489731\n",
      "Train Epoch: 1 [97500/124800 (78%)]\tLoss: 0.509005\n",
      "Train Epoch: 1 [100000/124800 (80%)]\tLoss: 0.396231\n",
      "Train Epoch: 1 [102500/124800 (82%)]\tLoss: 0.315778\n",
      "Train Epoch: 1 [105000/124800 (84%)]\tLoss: 0.297475\n",
      "Train Epoch: 1 [107500/124800 (86%)]\tLoss: 0.432980\n",
      "Train Epoch: 1 [110000/124800 (88%)]\tLoss: 0.426933\n",
      "Train Epoch: 1 [112500/124800 (90%)]\tLoss: 0.520621\n",
      "Train Epoch: 1 [115000/124800 (92%)]\tLoss: 0.283569\n",
      "Train Epoch: 1 [117500/124800 (94%)]\tLoss: 0.400484\n",
      "Train Epoch: 1 [120000/124800 (96%)]\tLoss: 0.273292\n",
      "Train Epoch: 1 [122500/124800 (98%)]\tLoss: 0.543678\n",
      "\n",
      "Test set: Avg. Loss: 0.3889, Accuracy: 18178/20800 (87%)\n",
      "\n",
      "Train Epoch: 2 [0/124800 (0%)]\tLoss: 0.399820\n",
      "Train Epoch: 2 [2500/124800 (2%)]\tLoss: 0.348007\n",
      "Train Epoch: 2 [5000/124800 (4%)]\tLoss: 0.319843\n",
      "Train Epoch: 2 [7500/124800 (6%)]\tLoss: 0.188148\n",
      "Train Epoch: 2 [10000/124800 (8%)]\tLoss: 0.643718\n",
      "Train Epoch: 2 [12500/124800 (10%)]\tLoss: 0.397887\n",
      "Train Epoch: 2 [15000/124800 (12%)]\tLoss: 0.283373\n",
      "Train Epoch: 2 [17500/124800 (14%)]\tLoss: 0.327550\n",
      "Train Epoch: 2 [20000/124800 (16%)]\tLoss: 0.384245\n",
      "Train Epoch: 2 [22500/124800 (18%)]\tLoss: 0.668752\n",
      "Train Epoch: 2 [25000/124800 (20%)]\tLoss: 0.471127\n",
      "Train Epoch: 2 [27500/124800 (22%)]\tLoss: 0.333228\n",
      "Train Epoch: 2 [30000/124800 (24%)]\tLoss: 0.463872\n",
      "Train Epoch: 2 [32500/124800 (26%)]\tLoss: 0.344117\n",
      "Train Epoch: 2 [35000/124800 (28%)]\tLoss: 0.288679\n",
      "Train Epoch: 2 [37500/124800 (30%)]\tLoss: 0.365205\n",
      "Train Epoch: 2 [40000/124800 (32%)]\tLoss: 0.346868\n",
      "Train Epoch: 2 [42500/124800 (34%)]\tLoss: 0.542242\n",
      "Train Epoch: 2 [45000/124800 (36%)]\tLoss: 0.377535\n",
      "Train Epoch: 2 [47500/124800 (38%)]\tLoss: 0.268165\n",
      "Train Epoch: 2 [50000/124800 (40%)]\tLoss: 0.216303\n",
      "Train Epoch: 2 [52500/124800 (42%)]\tLoss: 0.238186\n",
      "Train Epoch: 2 [55000/124800 (44%)]\tLoss: 0.364697\n",
      "Train Epoch: 2 [57500/124800 (46%)]\tLoss: 0.340801\n",
      "Train Epoch: 2 [60000/124800 (48%)]\tLoss: 0.255576\n",
      "Train Epoch: 2 [62500/124800 (50%)]\tLoss: 0.632062\n",
      "Train Epoch: 2 [65000/124800 (52%)]\tLoss: 0.375660\n",
      "Train Epoch: 2 [67500/124800 (54%)]\tLoss: 0.293070\n",
      "Train Epoch: 2 [70000/124800 (56%)]\tLoss: 0.218270\n",
      "Train Epoch: 2 [72500/124800 (58%)]\tLoss: 0.365772\n",
      "Train Epoch: 2 [75000/124800 (60%)]\tLoss: 0.304567\n",
      "Train Epoch: 2 [77500/124800 (62%)]\tLoss: 0.199229\n",
      "Train Epoch: 2 [80000/124800 (64%)]\tLoss: 0.320779\n",
      "Train Epoch: 2 [82500/124800 (66%)]\tLoss: 0.317189\n",
      "Train Epoch: 2 [85000/124800 (68%)]\tLoss: 0.304154\n",
      "Train Epoch: 2 [87500/124800 (70%)]\tLoss: 0.395778\n",
      "Train Epoch: 2 [90000/124800 (72%)]\tLoss: 0.373702\n",
      "Train Epoch: 2 [92500/124800 (74%)]\tLoss: 0.324230\n",
      "Train Epoch: 2 [95000/124800 (76%)]\tLoss: 0.121151\n",
      "Train Epoch: 2 [97500/124800 (78%)]\tLoss: 0.327952\n",
      "Train Epoch: 2 [100000/124800 (80%)]\tLoss: 0.476360\n",
      "Train Epoch: 2 [102500/124800 (82%)]\tLoss: 0.280859\n",
      "Train Epoch: 2 [105000/124800 (84%)]\tLoss: 0.129706\n",
      "Train Epoch: 2 [107500/124800 (86%)]\tLoss: 0.251344\n",
      "Train Epoch: 2 [110000/124800 (88%)]\tLoss: 0.610472\n",
      "Train Epoch: 2 [112500/124800 (90%)]\tLoss: 0.370941\n",
      "Train Epoch: 2 [115000/124800 (92%)]\tLoss: 0.240132\n",
      "Train Epoch: 2 [117500/124800 (94%)]\tLoss: 0.331059\n",
      "Train Epoch: 2 [120000/124800 (96%)]\tLoss: 0.280905\n",
      "Train Epoch: 2 [122500/124800 (98%)]\tLoss: 0.123310\n",
      "\n",
      "Test set: Avg. Loss: 0.3128, Accuracy: 18722/20800 (90%)\n",
      "\n",
      "Train Epoch: 3 [0/124800 (0%)]\tLoss: 0.321677\n",
      "Train Epoch: 3 [2500/124800 (2%)]\tLoss: 0.275472\n",
      "Train Epoch: 3 [5000/124800 (4%)]\tLoss: 0.218781\n",
      "Train Epoch: 3 [7500/124800 (6%)]\tLoss: 0.373477\n",
      "Train Epoch: 3 [10000/124800 (8%)]\tLoss: 0.201625\n",
      "Train Epoch: 3 [12500/124800 (10%)]\tLoss: 0.193920\n",
      "Train Epoch: 3 [15000/124800 (12%)]\tLoss: 0.273881\n",
      "Train Epoch: 3 [17500/124800 (14%)]\tLoss: 0.105712\n",
      "Train Epoch: 3 [20000/124800 (16%)]\tLoss: 0.253666\n",
      "Train Epoch: 3 [22500/124800 (18%)]\tLoss: 0.150489\n",
      "Train Epoch: 3 [25000/124800 (20%)]\tLoss: 0.353709\n",
      "Train Epoch: 3 [27500/124800 (22%)]\tLoss: 0.213561\n",
      "Train Epoch: 3 [30000/124800 (24%)]\tLoss: 0.197772\n",
      "Train Epoch: 3 [32500/124800 (26%)]\tLoss: 0.207589\n",
      "Train Epoch: 3 [35000/124800 (28%)]\tLoss: 0.311237\n",
      "Train Epoch: 3 [37500/124800 (30%)]\tLoss: 0.208815\n",
      "Train Epoch: 3 [40000/124800 (32%)]\tLoss: 0.370388\n",
      "Train Epoch: 3 [42500/124800 (34%)]\tLoss: 0.367485\n",
      "Train Epoch: 3 [45000/124800 (36%)]\tLoss: 0.217389\n",
      "Train Epoch: 3 [47500/124800 (38%)]\tLoss: 0.342003\n",
      "Train Epoch: 3 [50000/124800 (40%)]\tLoss: 0.106972\n",
      "Train Epoch: 3 [52500/124800 (42%)]\tLoss: 0.176023\n",
      "Train Epoch: 3 [55000/124800 (44%)]\tLoss: 0.488936\n",
      "Train Epoch: 3 [57500/124800 (46%)]\tLoss: 0.165777\n",
      "Train Epoch: 3 [60000/124800 (48%)]\tLoss: 0.144168\n",
      "Train Epoch: 3 [62500/124800 (50%)]\tLoss: 0.216861\n",
      "Train Epoch: 3 [65000/124800 (52%)]\tLoss: 0.320631\n",
      "Train Epoch: 3 [67500/124800 (54%)]\tLoss: 0.285895\n",
      "Train Epoch: 3 [70000/124800 (56%)]\tLoss: 0.369420\n",
      "Train Epoch: 3 [72500/124800 (58%)]\tLoss: 0.343381\n",
      "Train Epoch: 3 [75000/124800 (60%)]\tLoss: 0.095555\n",
      "Train Epoch: 3 [77500/124800 (62%)]\tLoss: 0.309896\n",
      "Train Epoch: 3 [80000/124800 (64%)]\tLoss: 0.196211\n",
      "Train Epoch: 3 [82500/124800 (66%)]\tLoss: 0.239385\n",
      "Train Epoch: 3 [85000/124800 (68%)]\tLoss: 0.334377\n",
      "Train Epoch: 3 [87500/124800 (70%)]\tLoss: 0.440024\n",
      "Train Epoch: 3 [90000/124800 (72%)]\tLoss: 0.431376\n",
      "Train Epoch: 3 [92500/124800 (74%)]\tLoss: 0.366579\n",
      "Train Epoch: 3 [95000/124800 (76%)]\tLoss: 0.121944\n",
      "Train Epoch: 3 [97500/124800 (78%)]\tLoss: 0.117898\n",
      "Train Epoch: 3 [100000/124800 (80%)]\tLoss: 0.279196\n",
      "Train Epoch: 3 [102500/124800 (82%)]\tLoss: 0.134396\n",
      "Train Epoch: 3 [105000/124800 (84%)]\tLoss: 0.118188\n",
      "Train Epoch: 3 [107500/124800 (86%)]\tLoss: 0.284234\n",
      "Train Epoch: 3 [110000/124800 (88%)]\tLoss: 0.403516\n",
      "Train Epoch: 3 [112500/124800 (90%)]\tLoss: 0.332156\n",
      "Train Epoch: 3 [115000/124800 (92%)]\tLoss: 0.175006\n",
      "Train Epoch: 3 [117500/124800 (94%)]\tLoss: 0.255506\n",
      "Train Epoch: 3 [120000/124800 (96%)]\tLoss: 0.164562\n",
      "Train Epoch: 3 [122500/124800 (98%)]\tLoss: 0.128483\n",
      "\n",
      "Test set: Avg. Loss: 0.2779, Accuracy: 18912/20800 (90%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importing the training data from the training file already present in EMNIST dataset\n",
    "training = torch.utils.data.DataLoader(torchvision.datasets.EMNIST(root = '/results/', split = 'letters' , train = True, download = True, transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),torchvision.transforms.Normalize((0.1307,),(0.3081,))])),batch_size = 50, shuffle = True)\n",
    "    # importing the testing data\n",
    "testing = torch.utils.data.DataLoader(torchvision.datasets.EMNIST(root = '/results/', split = 'letters' , train = False, download = True, transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),torchvision.transforms.Normalize((0.1307,),(0.3081,))])),batch_size = 1000, shuffle = True)   \n",
    "    # creating the neural network class. this class will consist of 3 convolutional layers, 3 max pooling layers and 2 linear layers for now. these layers will be connected by the widely used ReLU activation function. we will be using the Log softmax function for the output layer. these parameters will be varied during the subsequent runs to improve the accuracy\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "    #         input is 1X28X28, output is 26 letters\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=3, stride=1, padding=2)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size = 4, stride = 1, padding = 2)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n",
    "        self.conv3 = nn.Conv2d(20, 30, kernel_size = 3, stride = 1, padding = 2)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n",
    "        self.fc1 = nn.Linear(30 * 5 * 5, 150)\n",
    "        self.fc2 = nn.Linear(150, 27)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = x.view(-1, 30 * 5 *5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "            # defining the relationship between input and output sizes for usage afterwards\n",
    "def outputsize(in_size,kernel_size,stride,padding):\n",
    "    output = int((in_size-kernel_size+2*(padding)) / stride) + 1\n",
    "    return output\n",
    "    # defining the various hyperparameters\n",
    "epochs = 3 \n",
    "log_interval = 50\n",
    "network = Net()\n",
    "optimizer = optim.SGD(network.parameters(), lr = 0.01, momentum = 0.21)\n",
    "    # creating counters for our results i.e for training losses, test losses, no. of sample trained and no.of samples tested\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(training.dataset) for i in range(epochs+1)]\n",
    "        # defining the training network\n",
    "def train(epoch):\n",
    "    network.train()\n",
    "    for batch_idx, (data,target) in enumerate(training):\n",
    "    #       making all the gradients zero before the start of each epoch\n",
    "        optimizer.zero_grad()\n",
    "    #     input output and backpropagation steps along with the application of NLL loss function for now\n",
    "        output = network(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    #         printing the results after each mini batch has been trained during a particular epoch\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx*len(data), len(training.dataset), 100. *batch_idx / len(training), loss.item()))\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append((batch_idx*100) + ((epoch-1)*len(training.dataset)))\n",
    "            torch.save(network.state_dict(), '/results/model.pth')\n",
    "            torch.save(optimizer.state_dict(), '/results/optimizer.pth')\n",
    "    # defining the testing network\n",
    "def test():\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    losstest = []\n",
    "    #   calculating the loss for each sample and the accuracy\n",
    "    with torch.no_grad():\n",
    "        for data, target in testing:\n",
    "            output = network(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average = False).item()\n",
    "            pred = output.data.max(1, keepdim = True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    #         initialising the various parameters to calculate accuracy and print it\n",
    "        test_loss /= len(testing.dataset)\n",
    "        losstest.append(test_loss)\n",
    "        print('\\nTest set: Avg. Loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(testing.dataset), 100. *correct / len(testing.dataset)))\n",
    "    # initializing the entire network with the datasets and using the above functions to calculate the accuracy\n",
    "    # first checking the accuracy with random guesses(network uninitialised)\n",
    "test()\n",
    "for epoch in range(1,epochs+1):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. Loss: 3.2961, Accuracy: 789/20800 (3%)\n",
      "\n",
      "Train Epoch: 1 [0/124800 (0%)]\tLoss: 3.297979\n",
      "Train Epoch: 1 [2500/124800 (2%)]\tLoss: 3.296372\n",
      "Train Epoch: 1 [5000/124800 (4%)]\tLoss: 3.287894\n",
      "Train Epoch: 1 [7500/124800 (6%)]\tLoss: 3.282295\n",
      "Train Epoch: 1 [10000/124800 (8%)]\tLoss: 3.283067\n",
      "Train Epoch: 1 [12500/124800 (10%)]\tLoss: 3.267782\n",
      "Train Epoch: 1 [15000/124800 (12%)]\tLoss: 3.256880\n",
      "Train Epoch: 1 [17500/124800 (14%)]\tLoss: 3.222885\n",
      "Train Epoch: 1 [20000/124800 (16%)]\tLoss: 3.172037\n",
      "Train Epoch: 1 [22500/124800 (18%)]\tLoss: 2.871690\n",
      "Train Epoch: 1 [25000/124800 (20%)]\tLoss: 2.354845\n",
      "Train Epoch: 1 [27500/124800 (22%)]\tLoss: 2.296590\n",
      "Train Epoch: 1 [30000/124800 (24%)]\tLoss: 1.529847\n",
      "Train Epoch: 1 [32500/124800 (26%)]\tLoss: 1.760337\n",
      "Train Epoch: 1 [35000/124800 (28%)]\tLoss: 1.329441\n",
      "Train Epoch: 1 [37500/124800 (30%)]\tLoss: 1.071763\n",
      "Train Epoch: 1 [40000/124800 (32%)]\tLoss: 0.862050\n",
      "Train Epoch: 1 [42500/124800 (34%)]\tLoss: 1.265859\n",
      "Train Epoch: 1 [45000/124800 (36%)]\tLoss: 0.844067\n",
      "Train Epoch: 1 [47500/124800 (38%)]\tLoss: 1.124455\n",
      "Train Epoch: 1 [50000/124800 (40%)]\tLoss: 1.223665\n",
      "Train Epoch: 1 [52500/124800 (42%)]\tLoss: 0.953018\n",
      "Train Epoch: 1 [55000/124800 (44%)]\tLoss: 0.419308\n",
      "Train Epoch: 1 [57500/124800 (46%)]\tLoss: 0.909873\n",
      "Train Epoch: 1 [60000/124800 (48%)]\tLoss: 0.987627\n",
      "Train Epoch: 1 [62500/124800 (50%)]\tLoss: 0.821534\n",
      "Train Epoch: 1 [65000/124800 (52%)]\tLoss: 0.768226\n",
      "Train Epoch: 1 [67500/124800 (54%)]\tLoss: 0.470166\n",
      "Train Epoch: 1 [70000/124800 (56%)]\tLoss: 0.602929\n",
      "Train Epoch: 1 [72500/124800 (58%)]\tLoss: 0.395237\n",
      "Train Epoch: 1 [75000/124800 (60%)]\tLoss: 0.661341\n",
      "Train Epoch: 1 [77500/124800 (62%)]\tLoss: 0.484736\n",
      "Train Epoch: 1 [80000/124800 (64%)]\tLoss: 0.563467\n",
      "Train Epoch: 1 [82500/124800 (66%)]\tLoss: 0.671238\n",
      "Train Epoch: 1 [85000/124800 (68%)]\tLoss: 0.530540\n",
      "Train Epoch: 1 [87500/124800 (70%)]\tLoss: 0.591509\n",
      "Train Epoch: 1 [90000/124800 (72%)]\tLoss: 0.622392\n",
      "Train Epoch: 1 [92500/124800 (74%)]\tLoss: 0.295260\n",
      "Train Epoch: 1 [95000/124800 (76%)]\tLoss: 0.361610\n",
      "Train Epoch: 1 [97500/124800 (78%)]\tLoss: 0.692581\n",
      "Train Epoch: 1 [100000/124800 (80%)]\tLoss: 0.453358\n",
      "Train Epoch: 1 [102500/124800 (82%)]\tLoss: 0.494880\n",
      "Train Epoch: 1 [105000/124800 (84%)]\tLoss: 0.444968\n",
      "Train Epoch: 1 [107500/124800 (86%)]\tLoss: 0.218074\n",
      "Train Epoch: 1 [110000/124800 (88%)]\tLoss: 0.594221\n",
      "Train Epoch: 1 [112500/124800 (90%)]\tLoss: 0.558992\n",
      "Train Epoch: 1 [115000/124800 (92%)]\tLoss: 0.387943\n",
      "Train Epoch: 1 [117500/124800 (94%)]\tLoss: 0.469961\n",
      "Train Epoch: 1 [120000/124800 (96%)]\tLoss: 0.405040\n",
      "Train Epoch: 1 [122500/124800 (98%)]\tLoss: 0.305432\n",
      "\n",
      "Test set: Avg. Loss: 0.4371, Accuracy: 17884/20800 (85%)\n",
      "\n",
      "Train Epoch: 2 [0/124800 (0%)]\tLoss: 0.272820\n",
      "Train Epoch: 2 [2500/124800 (2%)]\tLoss: 0.341531\n",
      "Train Epoch: 2 [5000/124800 (4%)]\tLoss: 0.219858\n",
      "Train Epoch: 2 [7500/124800 (6%)]\tLoss: 0.375904\n",
      "Train Epoch: 2 [10000/124800 (8%)]\tLoss: 0.318750\n",
      "Train Epoch: 2 [12500/124800 (10%)]\tLoss: 0.525718\n",
      "Train Epoch: 2 [15000/124800 (12%)]\tLoss: 0.457327\n",
      "Train Epoch: 2 [17500/124800 (14%)]\tLoss: 0.490436\n",
      "Train Epoch: 2 [20000/124800 (16%)]\tLoss: 0.276650\n",
      "Train Epoch: 2 [22500/124800 (18%)]\tLoss: 0.536795\n",
      "Train Epoch: 2 [25000/124800 (20%)]\tLoss: 0.610986\n",
      "Train Epoch: 2 [27500/124800 (22%)]\tLoss: 0.512176\n",
      "Train Epoch: 2 [30000/124800 (24%)]\tLoss: 0.372643\n",
      "Train Epoch: 2 [32500/124800 (26%)]\tLoss: 0.235742\n",
      "Train Epoch: 2 [35000/124800 (28%)]\tLoss: 0.343728\n",
      "Train Epoch: 2 [37500/124800 (30%)]\tLoss: 0.253779\n",
      "Train Epoch: 2 [40000/124800 (32%)]\tLoss: 0.386647\n",
      "Train Epoch: 2 [42500/124800 (34%)]\tLoss: 0.362919\n",
      "Train Epoch: 2 [45000/124800 (36%)]\tLoss: 0.214420\n",
      "Train Epoch: 2 [47500/124800 (38%)]\tLoss: 0.758037\n",
      "Train Epoch: 2 [50000/124800 (40%)]\tLoss: 0.250708\n",
      "Train Epoch: 2 [52500/124800 (42%)]\tLoss: 0.292494\n",
      "Train Epoch: 2 [55000/124800 (44%)]\tLoss: 0.376012\n",
      "Train Epoch: 2 [57500/124800 (46%)]\tLoss: 0.247708\n",
      "Train Epoch: 2 [60000/124800 (48%)]\tLoss: 0.487505\n",
      "Train Epoch: 2 [62500/124800 (50%)]\tLoss: 0.381037\n",
      "Train Epoch: 2 [65000/124800 (52%)]\tLoss: 0.345086\n",
      "Train Epoch: 2 [67500/124800 (54%)]\tLoss: 0.233611\n",
      "Train Epoch: 2 [70000/124800 (56%)]\tLoss: 0.270358\n",
      "Train Epoch: 2 [72500/124800 (58%)]\tLoss: 0.577034\n",
      "Train Epoch: 2 [75000/124800 (60%)]\tLoss: 0.313236\n",
      "Train Epoch: 2 [77500/124800 (62%)]\tLoss: 0.350060\n",
      "Train Epoch: 2 [80000/124800 (64%)]\tLoss: 0.283828\n",
      "Train Epoch: 2 [82500/124800 (66%)]\tLoss: 0.235571\n",
      "Train Epoch: 2 [85000/124800 (68%)]\tLoss: 0.248986\n",
      "Train Epoch: 2 [87500/124800 (70%)]\tLoss: 0.244521\n",
      "Train Epoch: 2 [90000/124800 (72%)]\tLoss: 0.176916\n",
      "Train Epoch: 2 [92500/124800 (74%)]\tLoss: 0.406950\n",
      "Train Epoch: 2 [95000/124800 (76%)]\tLoss: 0.307583\n",
      "Train Epoch: 2 [97500/124800 (78%)]\tLoss: 0.393730\n",
      "Train Epoch: 2 [100000/124800 (80%)]\tLoss: 0.418486\n",
      "Train Epoch: 2 [102500/124800 (82%)]\tLoss: 0.348584\n",
      "Train Epoch: 2 [105000/124800 (84%)]\tLoss: 0.399667\n",
      "Train Epoch: 2 [107500/124800 (86%)]\tLoss: 0.235106\n",
      "Train Epoch: 2 [110000/124800 (88%)]\tLoss: 0.126759\n",
      "Train Epoch: 2 [112500/124800 (90%)]\tLoss: 0.356081\n",
      "Train Epoch: 2 [115000/124800 (92%)]\tLoss: 0.289705\n",
      "Train Epoch: 2 [117500/124800 (94%)]\tLoss: 0.603317\n",
      "Train Epoch: 2 [120000/124800 (96%)]\tLoss: 0.414237\n",
      "Train Epoch: 2 [122500/124800 (98%)]\tLoss: 0.322199\n",
      "\n",
      "Test set: Avg. Loss: 0.3197, Accuracy: 18565/20800 (89%)\n",
      "\n",
      "Train Epoch: 3 [0/124800 (0%)]\tLoss: 0.307964\n",
      "Train Epoch: 3 [2500/124800 (2%)]\tLoss: 0.448857\n",
      "Train Epoch: 3 [5000/124800 (4%)]\tLoss: 0.264002\n",
      "Train Epoch: 3 [7500/124800 (6%)]\tLoss: 0.336458\n",
      "Train Epoch: 3 [10000/124800 (8%)]\tLoss: 0.400551\n",
      "Train Epoch: 3 [12500/124800 (10%)]\tLoss: 0.186368\n",
      "Train Epoch: 3 [15000/124800 (12%)]\tLoss: 0.236665\n",
      "Train Epoch: 3 [17500/124800 (14%)]\tLoss: 0.235950\n",
      "Train Epoch: 3 [20000/124800 (16%)]\tLoss: 0.271544\n",
      "Train Epoch: 3 [22500/124800 (18%)]\tLoss: 0.186504\n",
      "Train Epoch: 3 [25000/124800 (20%)]\tLoss: 0.283644\n",
      "Train Epoch: 3 [27500/124800 (22%)]\tLoss: 0.252684\n",
      "Train Epoch: 3 [30000/124800 (24%)]\tLoss: 0.699173\n",
      "Train Epoch: 3 [32500/124800 (26%)]\tLoss: 0.213219\n",
      "Train Epoch: 3 [35000/124800 (28%)]\tLoss: 0.267150\n",
      "Train Epoch: 3 [37500/124800 (30%)]\tLoss: 0.305422\n",
      "Train Epoch: 3 [40000/124800 (32%)]\tLoss: 0.209816\n",
      "Train Epoch: 3 [42500/124800 (34%)]\tLoss: 0.268521\n",
      "Train Epoch: 3 [45000/124800 (36%)]\tLoss: 0.223648\n",
      "Train Epoch: 3 [47500/124800 (38%)]\tLoss: 0.164250\n",
      "Train Epoch: 3 [50000/124800 (40%)]\tLoss: 0.239289\n",
      "Train Epoch: 3 [52500/124800 (42%)]\tLoss: 0.212754\n",
      "Train Epoch: 3 [55000/124800 (44%)]\tLoss: 0.290916\n",
      "Train Epoch: 3 [57500/124800 (46%)]\tLoss: 0.183331\n",
      "Train Epoch: 3 [60000/124800 (48%)]\tLoss: 0.219870\n",
      "Train Epoch: 3 [62500/124800 (50%)]\tLoss: 0.369908\n",
      "Train Epoch: 3 [65000/124800 (52%)]\tLoss: 0.205141\n",
      "Train Epoch: 3 [67500/124800 (54%)]\tLoss: 0.183459\n",
      "Train Epoch: 3 [70000/124800 (56%)]\tLoss: 0.329784\n",
      "Train Epoch: 3 [72500/124800 (58%)]\tLoss: 0.256928\n",
      "Train Epoch: 3 [75000/124800 (60%)]\tLoss: 0.125156\n",
      "Train Epoch: 3 [77500/124800 (62%)]\tLoss: 0.292886\n",
      "Train Epoch: 3 [80000/124800 (64%)]\tLoss: 0.390713\n",
      "Train Epoch: 3 [82500/124800 (66%)]\tLoss: 0.334976\n",
      "Train Epoch: 3 [85000/124800 (68%)]\tLoss: 0.268458\n",
      "Train Epoch: 3 [87500/124800 (70%)]\tLoss: 0.183084\n",
      "Train Epoch: 3 [90000/124800 (72%)]\tLoss: 0.495388\n",
      "Train Epoch: 3 [92500/124800 (74%)]\tLoss: 0.204668\n",
      "Train Epoch: 3 [95000/124800 (76%)]\tLoss: 0.224638\n",
      "Train Epoch: 3 [97500/124800 (78%)]\tLoss: 0.342761\n",
      "Train Epoch: 3 [100000/124800 (80%)]\tLoss: 0.242302\n",
      "Train Epoch: 3 [102500/124800 (82%)]\tLoss: 0.124174\n",
      "Train Epoch: 3 [105000/124800 (84%)]\tLoss: 0.414556\n",
      "Train Epoch: 3 [107500/124800 (86%)]\tLoss: 0.193687\n",
      "Train Epoch: 3 [110000/124800 (88%)]\tLoss: 0.188554\n",
      "Train Epoch: 3 [112500/124800 (90%)]\tLoss: 0.125299\n",
      "Train Epoch: 3 [115000/124800 (92%)]\tLoss: 0.213194\n",
      "Train Epoch: 3 [117500/124800 (94%)]\tLoss: 0.275199\n",
      "Train Epoch: 3 [120000/124800 (96%)]\tLoss: 0.370306\n",
      "Train Epoch: 3 [122500/124800 (98%)]\tLoss: 0.101190\n",
      "\n",
      "Test set: Avg. Loss: 0.2722, Accuracy: 18954/20800 (91%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importing the training data from the training file already present in EMNIST dataset\n",
    "training = torch.utils.data.DataLoader(torchvision.datasets.EMNIST(root = '/results/', split = 'letters' , train = True, download = True, transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),torchvision.transforms.Normalize((0.1307,),(0.3081,))])),batch_size = 50, shuffle = True)\n",
    "    # importing the testing data\n",
    "testing = torch.utils.data.DataLoader(torchvision.datasets.EMNIST(root = '/results/', split = 'letters' , train = False, download = True, transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),torchvision.transforms.Normalize((0.1307,),(0.3081,))])),batch_size = 1000, shuffle = True)   \n",
    "    # creating the neural network class. this class will consist of 3 convolutional layers, 3 max pooling layers and 2 linear layers for now. these layers will be connected by the widely used ReLU activation function. we will be using the Log softmax function for the output layer. these parameters will be varied during the subsequent runs to improve the accuracy\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "    #         input is 1X28X28, output is 26 letters\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=4, stride=1, padding=2)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size = 3, stride = 1, padding = 2)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n",
    "        self.conv3 = nn.Conv2d(20, 30, kernel_size = 3, stride = 1, padding = 2)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n",
    "        self.fc1 = nn.Linear(30 * 5 * 5, 150)\n",
    "        self.fc2 = nn.Linear(150, 27)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = x.view(-1, 30 * 5 *5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "            # defining the relationship between input and output sizes for usage afterwards\n",
    "def outputsize(in_size,kernel_size,stride,padding):\n",
    "    output = int((in_size-kernel_size+2*(padding)) / stride) + 1\n",
    "    return output\n",
    "    # defining the various hyperparameters\n",
    "epochs = 3 \n",
    "log_interval = 50\n",
    "network = Net()\n",
    "optimizer = optim.SGD(network.parameters(), lr = 0.01, momentum = 0.21)\n",
    "    # creating counters for our results i.e for training losses, test losses, no. of sample trained and no.of samples tested\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(training.dataset) for i in range(epochs+1)]\n",
    "        # defining the training network\n",
    "def train(epoch):\n",
    "    network.train()\n",
    "    for batch_idx, (data,target) in enumerate(training):\n",
    "    #       making all the gradients zero before the start of each epoch\n",
    "        optimizer.zero_grad()\n",
    "    #     input output and backpropagation steps along with the application of NLL loss function for now\n",
    "        output = network(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    #         printing the results after each mini batch has been trained during a particular epoch\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx*len(data), len(training.dataset), 100. *batch_idx / len(training), loss.item()))\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append((batch_idx*100) + ((epoch-1)*len(training.dataset)))\n",
    "            torch.save(network.state_dict(), '/results/model.pth')\n",
    "            torch.save(optimizer.state_dict(), '/results/optimizer.pth')\n",
    "    # defining the testing network\n",
    "def test():\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    losstest = []\n",
    "    #   calculating the loss for each sample and the accuracy\n",
    "    with torch.no_grad():\n",
    "        for data, target in testing:\n",
    "            output = network(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average = False).item()\n",
    "            pred = output.data.max(1, keepdim = True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    #         initialising the various parameters to calculate accuracy and print it\n",
    "        test_loss /= len(testing.dataset)\n",
    "        losstest.append(test_loss)\n",
    "        print('\\nTest set: Avg. Loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(testing.dataset), 100. *correct / len(testing.dataset)))\n",
    "    # initializing the entire network with the datasets and using the above functions to calculate the accuracy\n",
    "    # first checking the accuracy with random guesses(network uninitialised)\n",
    "test()\n",
    "for epoch in range(1,epochs+1):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. Loss: 3.2935, Accuracy: 801/20800 (3%)\n",
      "\n",
      "Train Epoch: 1 [0/124800 (0%)]\tLoss: 3.294183\n",
      "Train Epoch: 1 [2500/124800 (2%)]\tLoss: 3.296167\n",
      "Train Epoch: 1 [5000/124800 (4%)]\tLoss: 3.288988\n",
      "Train Epoch: 1 [7500/124800 (6%)]\tLoss: 3.284645\n",
      "Train Epoch: 1 [10000/124800 (8%)]\tLoss: 3.272374\n",
      "Train Epoch: 1 [12500/124800 (10%)]\tLoss: 3.266382\n",
      "Train Epoch: 1 [15000/124800 (12%)]\tLoss: 3.236729\n",
      "Train Epoch: 1 [17500/124800 (14%)]\tLoss: 3.247952\n",
      "Train Epoch: 1 [20000/124800 (16%)]\tLoss: 3.229108\n",
      "Train Epoch: 1 [22500/124800 (18%)]\tLoss: 3.084645\n",
      "Train Epoch: 1 [25000/124800 (20%)]\tLoss: 2.461247\n",
      "Train Epoch: 1 [27500/124800 (22%)]\tLoss: 2.134232\n",
      "Train Epoch: 1 [30000/124800 (24%)]\tLoss: 2.039891\n",
      "Train Epoch: 1 [32500/124800 (26%)]\tLoss: 1.731627\n",
      "Train Epoch: 1 [35000/124800 (28%)]\tLoss: 1.643221\n",
      "Train Epoch: 1 [37500/124800 (30%)]\tLoss: 1.174082\n",
      "Train Epoch: 1 [40000/124800 (32%)]\tLoss: 1.498331\n",
      "Train Epoch: 1 [42500/124800 (34%)]\tLoss: 0.979125\n",
      "Train Epoch: 1 [45000/124800 (36%)]\tLoss: 0.965120\n",
      "Train Epoch: 1 [47500/124800 (38%)]\tLoss: 1.051350\n",
      "Train Epoch: 1 [50000/124800 (40%)]\tLoss: 0.945993\n",
      "Train Epoch: 1 [52500/124800 (42%)]\tLoss: 0.704423\n",
      "Train Epoch: 1 [55000/124800 (44%)]\tLoss: 0.878051\n",
      "Train Epoch: 1 [57500/124800 (46%)]\tLoss: 0.526813\n",
      "Train Epoch: 1 [60000/124800 (48%)]\tLoss: 0.601524\n",
      "Train Epoch: 1 [62500/124800 (50%)]\tLoss: 0.674814\n",
      "Train Epoch: 1 [65000/124800 (52%)]\tLoss: 0.705125\n",
      "Train Epoch: 1 [67500/124800 (54%)]\tLoss: 0.552465\n",
      "Train Epoch: 1 [70000/124800 (56%)]\tLoss: 0.452048\n",
      "Train Epoch: 1 [72500/124800 (58%)]\tLoss: 0.906401\n",
      "Train Epoch: 1 [75000/124800 (60%)]\tLoss: 0.670896\n",
      "Train Epoch: 1 [77500/124800 (62%)]\tLoss: 0.810469\n",
      "Train Epoch: 1 [80000/124800 (64%)]\tLoss: 0.598061\n",
      "Train Epoch: 1 [82500/124800 (66%)]\tLoss: 0.555518\n",
      "Train Epoch: 1 [85000/124800 (68%)]\tLoss: 0.379000\n",
      "Train Epoch: 1 [87500/124800 (70%)]\tLoss: 0.421403\n",
      "Train Epoch: 1 [90000/124800 (72%)]\tLoss: 0.776291\n",
      "Train Epoch: 1 [92500/124800 (74%)]\tLoss: 0.377760\n",
      "Train Epoch: 1 [95000/124800 (76%)]\tLoss: 0.259166\n",
      "Train Epoch: 1 [97500/124800 (78%)]\tLoss: 0.221506\n",
      "Train Epoch: 1 [100000/124800 (80%)]\tLoss: 0.497392\n",
      "Train Epoch: 1 [102500/124800 (82%)]\tLoss: 0.586221\n",
      "Train Epoch: 1 [105000/124800 (84%)]\tLoss: 0.526939\n",
      "Train Epoch: 1 [107500/124800 (86%)]\tLoss: 0.692621\n",
      "Train Epoch: 1 [110000/124800 (88%)]\tLoss: 0.404584\n",
      "Train Epoch: 1 [112500/124800 (90%)]\tLoss: 0.254572\n",
      "Train Epoch: 1 [115000/124800 (92%)]\tLoss: 0.474231\n",
      "Train Epoch: 1 [117500/124800 (94%)]\tLoss: 0.219332\n",
      "Train Epoch: 1 [120000/124800 (96%)]\tLoss: 0.481898\n",
      "Train Epoch: 1 [122500/124800 (98%)]\tLoss: 0.247490\n",
      "\n",
      "Test set: Avg. Loss: 0.4374, Accuracy: 17890/20800 (86%)\n",
      "\n",
      "Train Epoch: 2 [0/124800 (0%)]\tLoss: 0.521045\n",
      "Train Epoch: 2 [2500/124800 (2%)]\tLoss: 0.466483\n",
      "Train Epoch: 2 [5000/124800 (4%)]\tLoss: 0.477741\n",
      "Train Epoch: 2 [7500/124800 (6%)]\tLoss: 0.326735\n",
      "Train Epoch: 2 [10000/124800 (8%)]\tLoss: 0.405982\n",
      "Train Epoch: 2 [12500/124800 (10%)]\tLoss: 0.253390\n",
      "Train Epoch: 2 [15000/124800 (12%)]\tLoss: 0.481460\n",
      "Train Epoch: 2 [17500/124800 (14%)]\tLoss: 0.270345\n",
      "Train Epoch: 2 [20000/124800 (16%)]\tLoss: 0.350999\n",
      "Train Epoch: 2 [22500/124800 (18%)]\tLoss: 0.650383\n",
      "Train Epoch: 2 [25000/124800 (20%)]\tLoss: 0.618918\n",
      "Train Epoch: 2 [27500/124800 (22%)]\tLoss: 0.380525\n",
      "Train Epoch: 2 [30000/124800 (24%)]\tLoss: 0.240501\n",
      "Train Epoch: 2 [32500/124800 (26%)]\tLoss: 0.218896\n",
      "Train Epoch: 2 [35000/124800 (28%)]\tLoss: 0.374175\n",
      "Train Epoch: 2 [37500/124800 (30%)]\tLoss: 0.292945\n",
      "Train Epoch: 2 [40000/124800 (32%)]\tLoss: 0.177204\n",
      "Train Epoch: 2 [42500/124800 (34%)]\tLoss: 0.353526\n",
      "Train Epoch: 2 [45000/124800 (36%)]\tLoss: 0.166913\n",
      "Train Epoch: 2 [47500/124800 (38%)]\tLoss: 0.326504\n",
      "Train Epoch: 2 [50000/124800 (40%)]\tLoss: 0.260081\n",
      "Train Epoch: 2 [52500/124800 (42%)]\tLoss: 0.600637\n",
      "Train Epoch: 2 [55000/124800 (44%)]\tLoss: 0.418205\n",
      "Train Epoch: 2 [57500/124800 (46%)]\tLoss: 0.344734\n",
      "Train Epoch: 2 [60000/124800 (48%)]\tLoss: 0.261467\n",
      "Train Epoch: 2 [62500/124800 (50%)]\tLoss: 0.370830\n",
      "Train Epoch: 2 [65000/124800 (52%)]\tLoss: 0.341709\n",
      "Train Epoch: 2 [67500/124800 (54%)]\tLoss: 0.247287\n",
      "Train Epoch: 2 [70000/124800 (56%)]\tLoss: 0.323492\n",
      "Train Epoch: 2 [72500/124800 (58%)]\tLoss: 0.168126\n",
      "Train Epoch: 2 [75000/124800 (60%)]\tLoss: 0.382363\n",
      "Train Epoch: 2 [77500/124800 (62%)]\tLoss: 0.217454\n",
      "Train Epoch: 2 [80000/124800 (64%)]\tLoss: 0.575365\n",
      "Train Epoch: 2 [82500/124800 (66%)]\tLoss: 0.429074\n",
      "Train Epoch: 2 [85000/124800 (68%)]\tLoss: 0.343847\n",
      "Train Epoch: 2 [87500/124800 (70%)]\tLoss: 0.181925\n",
      "Train Epoch: 2 [90000/124800 (72%)]\tLoss: 0.366958\n",
      "Train Epoch: 2 [92500/124800 (74%)]\tLoss: 0.438744\n",
      "Train Epoch: 2 [95000/124800 (76%)]\tLoss: 0.465486\n",
      "Train Epoch: 2 [97500/124800 (78%)]\tLoss: 0.245448\n",
      "Train Epoch: 2 [100000/124800 (80%)]\tLoss: 0.155538\n",
      "Train Epoch: 2 [102500/124800 (82%)]\tLoss: 0.292926\n",
      "Train Epoch: 2 [105000/124800 (84%)]\tLoss: 0.333171\n",
      "Train Epoch: 2 [107500/124800 (86%)]\tLoss: 0.148630\n",
      "Train Epoch: 2 [110000/124800 (88%)]\tLoss: 0.234310\n",
      "Train Epoch: 2 [112500/124800 (90%)]\tLoss: 0.163675\n",
      "Train Epoch: 2 [115000/124800 (92%)]\tLoss: 0.505240\n",
      "Train Epoch: 2 [117500/124800 (94%)]\tLoss: 0.472827\n",
      "Train Epoch: 2 [120000/124800 (96%)]\tLoss: 0.413219\n",
      "Train Epoch: 2 [122500/124800 (98%)]\tLoss: 0.274273\n",
      "\n",
      "Test set: Avg. Loss: 0.2984, Accuracy: 18773/20800 (90%)\n",
      "\n",
      "Train Epoch: 3 [0/124800 (0%)]\tLoss: 0.206310\n",
      "Train Epoch: 3 [2500/124800 (2%)]\tLoss: 0.263736\n",
      "Train Epoch: 3 [5000/124800 (4%)]\tLoss: 0.420725\n",
      "Train Epoch: 3 [7500/124800 (6%)]\tLoss: 0.749316\n",
      "Train Epoch: 3 [10000/124800 (8%)]\tLoss: 0.154141\n",
      "Train Epoch: 3 [12500/124800 (10%)]\tLoss: 0.268631\n",
      "Train Epoch: 3 [15000/124800 (12%)]\tLoss: 0.335026\n",
      "Train Epoch: 3 [17500/124800 (14%)]\tLoss: 0.276125\n",
      "Train Epoch: 3 [20000/124800 (16%)]\tLoss: 0.165605\n",
      "Train Epoch: 3 [22500/124800 (18%)]\tLoss: 0.384515\n",
      "Train Epoch: 3 [25000/124800 (20%)]\tLoss: 0.267288\n",
      "Train Epoch: 3 [27500/124800 (22%)]\tLoss: 0.567323\n",
      "Train Epoch: 3 [30000/124800 (24%)]\tLoss: 0.171807\n",
      "Train Epoch: 3 [32500/124800 (26%)]\tLoss: 0.266623\n",
      "Train Epoch: 3 [35000/124800 (28%)]\tLoss: 0.125978\n",
      "Train Epoch: 3 [37500/124800 (30%)]\tLoss: 0.224263\n",
      "Train Epoch: 3 [40000/124800 (32%)]\tLoss: 0.349804\n",
      "Train Epoch: 3 [42500/124800 (34%)]\tLoss: 0.388745\n",
      "Train Epoch: 3 [45000/124800 (36%)]\tLoss: 0.299216\n",
      "Train Epoch: 3 [47500/124800 (38%)]\tLoss: 0.208041\n",
      "Train Epoch: 3 [50000/124800 (40%)]\tLoss: 0.260668\n",
      "Train Epoch: 3 [52500/124800 (42%)]\tLoss: 0.232033\n",
      "Train Epoch: 3 [55000/124800 (44%)]\tLoss: 0.302282\n",
      "Train Epoch: 3 [57500/124800 (46%)]\tLoss: 0.190403\n",
      "Train Epoch: 3 [60000/124800 (48%)]\tLoss: 0.399079\n",
      "Train Epoch: 3 [62500/124800 (50%)]\tLoss: 0.353274\n",
      "Train Epoch: 3 [65000/124800 (52%)]\tLoss: 0.227806\n",
      "Train Epoch: 3 [67500/124800 (54%)]\tLoss: 0.215034\n",
      "Train Epoch: 3 [70000/124800 (56%)]\tLoss: 0.226635\n",
      "Train Epoch: 3 [72500/124800 (58%)]\tLoss: 0.230758\n",
      "Train Epoch: 3 [75000/124800 (60%)]\tLoss: 0.265794\n",
      "Train Epoch: 3 [77500/124800 (62%)]\tLoss: 0.291743\n",
      "Train Epoch: 3 [80000/124800 (64%)]\tLoss: 0.380724\n",
      "Train Epoch: 3 [82500/124800 (66%)]\tLoss: 0.260041\n",
      "Train Epoch: 3 [85000/124800 (68%)]\tLoss: 0.089181\n",
      "Train Epoch: 3 [87500/124800 (70%)]\tLoss: 0.172219\n",
      "Train Epoch: 3 [90000/124800 (72%)]\tLoss: 0.185334\n",
      "Train Epoch: 3 [92500/124800 (74%)]\tLoss: 0.459071\n",
      "Train Epoch: 3 [95000/124800 (76%)]\tLoss: 0.345374\n",
      "Train Epoch: 3 [97500/124800 (78%)]\tLoss: 0.393890\n",
      "Train Epoch: 3 [100000/124800 (80%)]\tLoss: 0.406389\n",
      "Train Epoch: 3 [102500/124800 (82%)]\tLoss: 0.197986\n",
      "Train Epoch: 3 [105000/124800 (84%)]\tLoss: 0.188115\n",
      "Train Epoch: 3 [107500/124800 (86%)]\tLoss: 0.230343\n",
      "Train Epoch: 3 [110000/124800 (88%)]\tLoss: 0.160905\n",
      "Train Epoch: 3 [112500/124800 (90%)]\tLoss: 0.286744\n",
      "Train Epoch: 3 [115000/124800 (92%)]\tLoss: 0.190045\n",
      "Train Epoch: 3 [117500/124800 (94%)]\tLoss: 0.327141\n",
      "Train Epoch: 3 [120000/124800 (96%)]\tLoss: 0.213304\n",
      "Train Epoch: 3 [122500/124800 (98%)]\tLoss: 0.229227\n",
      "\n",
      "Test set: Avg. Loss: 0.2836, Accuracy: 18807/20800 (90%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importing the training data from the training file already present in EMNIST dataset\n",
    "training = torch.utils.data.DataLoader(torchvision.datasets.EMNIST(root = '/results/', split = 'letters' , train = True, download = True, transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),torchvision.transforms.Normalize((0.1307,),(0.3081,))])),batch_size = 50, shuffle = True)\n",
    "    # importing the testing data\n",
    "testing = torch.utils.data.DataLoader(torchvision.datasets.EMNIST(root = '/results/', split = 'letters' , train = False, download = True, transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),torchvision.transforms.Normalize((0.1307,),(0.3081,))])),batch_size = 1000, shuffle = True)   \n",
    "    # creating the neural network class. this class will consist of 3 convolutional layers, 3 max pooling layers and 2 linear layers for now. these layers will be connected by the widely used ReLU activation function. we will be using the Log softmax function for the output layer. these parameters will be varied during the subsequent runs to improve the accuracy\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "    #         input is 1X28X28, output is 26 letters\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=3, stride=1, padding=2)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size = 4, stride = 1, padding = 2)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n",
    "        self.conv3 = nn.Conv2d(20, 30, kernel_size = 4, stride = 1, padding = 2)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n",
    "        self.fc1 = nn.Linear(30 * 4 * 4, 150)\n",
    "        self.fc2 = nn.Linear(150, 27)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = x.view(-1, 30 * 4 *4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "            # defining the relationship between input and output sizes for usage afterwards\n",
    "def outputsize(in_size,kernel_size,stride,padding):\n",
    "    output = int((in_size-kernel_size+2*(padding)) / stride) + 1\n",
    "    return output\n",
    "    # defining the various hyperparameters\n",
    "epochs = 3 \n",
    "log_interval = 50\n",
    "network = Net()\n",
    "optimizer = optim.SGD(network.parameters(), lr = 0.01, momentum = 0.21)\n",
    "    # creating counters for our results i.e for training losses, test losses, no. of sample trained and no.of samples tested\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(training.dataset) for i in range(epochs+1)]\n",
    "        # defining the training network\n",
    "def train(epoch):\n",
    "    network.train()\n",
    "    for batch_idx, (data,target) in enumerate(training):\n",
    "    #       making all the gradients zero before the start of each epoch\n",
    "        optimizer.zero_grad()\n",
    "    #     input output and backpropagation steps along with the application of NLL loss function for now\n",
    "        output = network(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    #         printing the results after each mini batch has been trained during a particular epoch\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx*len(data), len(training.dataset), 100. *batch_idx / len(training), loss.item()))\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append((batch_idx*100) + ((epoch-1)*len(training.dataset)))\n",
    "            torch.save(network.state_dict(), '/results/model.pth')\n",
    "            torch.save(optimizer.state_dict(), '/results/optimizer.pth')\n",
    "    # defining the testing network\n",
    "def test():\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    losstest = []\n",
    "    #   calculating the loss for each sample and the accuracy\n",
    "    with torch.no_grad():\n",
    "        for data, target in testing:\n",
    "            output = network(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average = False).item()\n",
    "            pred = output.data.max(1, keepdim = True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    #         initialising the various parameters to calculate accuracy and print it\n",
    "        test_loss /= len(testing.dataset)\n",
    "        losstest.append(test_loss)\n",
    "        print('\\nTest set: Avg. Loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(testing.dataset), 100. *correct / len(testing.dataset)))\n",
    "    # initializing the entire network with the datasets and using the above functions to calculate the accuracy\n",
    "    # first checking the accuracy with random guesses(network uninitialised)\n",
    "test()\n",
    "for epoch in range(1,epochs+1):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. Loss: 3.2982, Accuracy: 734/20800 (3%)\n",
      "\n",
      "Train Epoch: 1 [0/124800 (0%)]\tLoss: 3.310565\n",
      "Train Epoch: 1 [2500/124800 (2%)]\tLoss: 3.295566\n",
      "Train Epoch: 1 [5000/124800 (4%)]\tLoss: 3.290817\n",
      "Train Epoch: 1 [7500/124800 (6%)]\tLoss: 3.265923\n",
      "Train Epoch: 1 [10000/124800 (8%)]\tLoss: 3.242192\n",
      "Train Epoch: 1 [12500/124800 (10%)]\tLoss: 3.231559\n",
      "Train Epoch: 1 [15000/124800 (12%)]\tLoss: 3.128833\n",
      "Train Epoch: 1 [17500/124800 (14%)]\tLoss: 2.599334\n",
      "Train Epoch: 1 [20000/124800 (16%)]\tLoss: 2.124582\n",
      "Train Epoch: 1 [22500/124800 (18%)]\tLoss: 1.920908\n",
      "Train Epoch: 1 [25000/124800 (20%)]\tLoss: 1.760690\n",
      "Train Epoch: 1 [27500/124800 (22%)]\tLoss: 1.557215\n",
      "Train Epoch: 1 [30000/124800 (24%)]\tLoss: 1.622482\n",
      "Train Epoch: 1 [32500/124800 (26%)]\tLoss: 1.329498\n",
      "Train Epoch: 1 [35000/124800 (28%)]\tLoss: 1.266629\n",
      "Train Epoch: 1 [37500/124800 (30%)]\tLoss: 1.368212\n",
      "Train Epoch: 1 [40000/124800 (32%)]\tLoss: 0.860116\n",
      "Train Epoch: 1 [42500/124800 (34%)]\tLoss: 1.225881\n",
      "Train Epoch: 1 [45000/124800 (36%)]\tLoss: 0.573659\n",
      "Train Epoch: 1 [47500/124800 (38%)]\tLoss: 0.991124\n",
      "Train Epoch: 1 [50000/124800 (40%)]\tLoss: 0.841548\n",
      "Train Epoch: 1 [52500/124800 (42%)]\tLoss: 0.786568\n",
      "Train Epoch: 1 [55000/124800 (44%)]\tLoss: 0.775914\n",
      "Train Epoch: 1 [57500/124800 (46%)]\tLoss: 0.771042\n",
      "Train Epoch: 1 [60000/124800 (48%)]\tLoss: 1.084092\n",
      "Train Epoch: 1 [62500/124800 (50%)]\tLoss: 0.445924\n",
      "Train Epoch: 1 [65000/124800 (52%)]\tLoss: 0.953200\n",
      "Train Epoch: 1 [67500/124800 (54%)]\tLoss: 0.335823\n",
      "Train Epoch: 1 [70000/124800 (56%)]\tLoss: 0.591610\n",
      "Train Epoch: 1 [72500/124800 (58%)]\tLoss: 0.532559\n",
      "Train Epoch: 1 [75000/124800 (60%)]\tLoss: 0.480838\n",
      "Train Epoch: 1 [77500/124800 (62%)]\tLoss: 0.396895\n",
      "Train Epoch: 1 [80000/124800 (64%)]\tLoss: 0.553919\n",
      "Train Epoch: 1 [82500/124800 (66%)]\tLoss: 0.426278\n",
      "Train Epoch: 1 [85000/124800 (68%)]\tLoss: 0.629523\n",
      "Train Epoch: 1 [87500/124800 (70%)]\tLoss: 0.297483\n",
      "Train Epoch: 1 [90000/124800 (72%)]\tLoss: 0.707457\n",
      "Train Epoch: 1 [92500/124800 (74%)]\tLoss: 0.547690\n",
      "Train Epoch: 1 [95000/124800 (76%)]\tLoss: 0.336673\n",
      "Train Epoch: 1 [97500/124800 (78%)]\tLoss: 0.490469\n",
      "Train Epoch: 1 [100000/124800 (80%)]\tLoss: 0.481501\n",
      "Train Epoch: 1 [102500/124800 (82%)]\tLoss: 0.449209\n",
      "Train Epoch: 1 [105000/124800 (84%)]\tLoss: 0.255278\n",
      "Train Epoch: 1 [107500/124800 (86%)]\tLoss: 0.410278\n",
      "Train Epoch: 1 [110000/124800 (88%)]\tLoss: 0.525389\n",
      "Train Epoch: 1 [112500/124800 (90%)]\tLoss: 0.607709\n",
      "Train Epoch: 1 [115000/124800 (92%)]\tLoss: 0.485513\n",
      "Train Epoch: 1 [117500/124800 (94%)]\tLoss: 0.446646\n",
      "Train Epoch: 1 [120000/124800 (96%)]\tLoss: 0.453952\n",
      "Train Epoch: 1 [122500/124800 (98%)]\tLoss: 0.392508\n",
      "\n",
      "Test set: Avg. Loss: 0.4093, Accuracy: 18128/20800 (87%)\n",
      "\n",
      "Train Epoch: 2 [0/124800 (0%)]\tLoss: 0.402759\n",
      "Train Epoch: 2 [2500/124800 (2%)]\tLoss: 0.717228\n",
      "Train Epoch: 2 [5000/124800 (4%)]\tLoss: 0.482664\n",
      "Train Epoch: 2 [7500/124800 (6%)]\tLoss: 0.444994\n",
      "Train Epoch: 2 [10000/124800 (8%)]\tLoss: 0.438893\n",
      "Train Epoch: 2 [12500/124800 (10%)]\tLoss: 0.266376\n",
      "Train Epoch: 2 [15000/124800 (12%)]\tLoss: 0.399012\n",
      "Train Epoch: 2 [17500/124800 (14%)]\tLoss: 0.307342\n",
      "Train Epoch: 2 [20000/124800 (16%)]\tLoss: 0.283054\n",
      "Train Epoch: 2 [22500/124800 (18%)]\tLoss: 0.495256\n",
      "Train Epoch: 2 [25000/124800 (20%)]\tLoss: 0.354886\n",
      "Train Epoch: 2 [27500/124800 (22%)]\tLoss: 0.346543\n",
      "Train Epoch: 2 [30000/124800 (24%)]\tLoss: 0.247460\n",
      "Train Epoch: 2 [32500/124800 (26%)]\tLoss: 0.460502\n",
      "Train Epoch: 2 [35000/124800 (28%)]\tLoss: 0.227820\n",
      "Train Epoch: 2 [37500/124800 (30%)]\tLoss: 0.297036\n",
      "Train Epoch: 2 [40000/124800 (32%)]\tLoss: 0.336435\n",
      "Train Epoch: 2 [42500/124800 (34%)]\tLoss: 0.319435\n",
      "Train Epoch: 2 [45000/124800 (36%)]\tLoss: 0.307621\n",
      "Train Epoch: 2 [47500/124800 (38%)]\tLoss: 0.277326\n",
      "Train Epoch: 2 [50000/124800 (40%)]\tLoss: 0.188707\n",
      "Train Epoch: 2 [52500/124800 (42%)]\tLoss: 0.295114\n",
      "Train Epoch: 2 [55000/124800 (44%)]\tLoss: 0.231575\n",
      "Train Epoch: 2 [57500/124800 (46%)]\tLoss: 0.578279\n",
      "Train Epoch: 2 [60000/124800 (48%)]\tLoss: 0.249464\n",
      "Train Epoch: 2 [62500/124800 (50%)]\tLoss: 0.374810\n",
      "Train Epoch: 2 [65000/124800 (52%)]\tLoss: 0.326818\n",
      "Train Epoch: 2 [67500/124800 (54%)]\tLoss: 0.457685\n",
      "Train Epoch: 2 [70000/124800 (56%)]\tLoss: 0.309594\n",
      "Train Epoch: 2 [72500/124800 (58%)]\tLoss: 0.284362\n",
      "Train Epoch: 2 [75000/124800 (60%)]\tLoss: 0.329302\n",
      "Train Epoch: 2 [77500/124800 (62%)]\tLoss: 0.363047\n",
      "Train Epoch: 2 [80000/124800 (64%)]\tLoss: 0.638935\n",
      "Train Epoch: 2 [82500/124800 (66%)]\tLoss: 0.258822\n",
      "Train Epoch: 2 [85000/124800 (68%)]\tLoss: 0.357004\n",
      "Train Epoch: 2 [87500/124800 (70%)]\tLoss: 0.280878\n",
      "Train Epoch: 2 [90000/124800 (72%)]\tLoss: 0.281031\n",
      "Train Epoch: 2 [92500/124800 (74%)]\tLoss: 0.362775\n",
      "Train Epoch: 2 [95000/124800 (76%)]\tLoss: 0.284389\n",
      "Train Epoch: 2 [97500/124800 (78%)]\tLoss: 0.302820\n",
      "Train Epoch: 2 [100000/124800 (80%)]\tLoss: 0.184724\n",
      "Train Epoch: 2 [102500/124800 (82%)]\tLoss: 0.196736\n",
      "Train Epoch: 2 [105000/124800 (84%)]\tLoss: 0.138164\n",
      "Train Epoch: 2 [107500/124800 (86%)]\tLoss: 0.276431\n",
      "Train Epoch: 2 [110000/124800 (88%)]\tLoss: 0.202097\n",
      "Train Epoch: 2 [112500/124800 (90%)]\tLoss: 0.308880\n",
      "Train Epoch: 2 [115000/124800 (92%)]\tLoss: 0.253259\n",
      "Train Epoch: 2 [117500/124800 (94%)]\tLoss: 0.232518\n",
      "Train Epoch: 2 [120000/124800 (96%)]\tLoss: 0.334712\n",
      "Train Epoch: 2 [122500/124800 (98%)]\tLoss: 0.340664\n",
      "\n",
      "Test set: Avg. Loss: 0.3123, Accuracy: 18678/20800 (89%)\n",
      "\n",
      "Train Epoch: 3 [0/124800 (0%)]\tLoss: 0.153784\n",
      "Train Epoch: 3 [2500/124800 (2%)]\tLoss: 0.204811\n",
      "Train Epoch: 3 [5000/124800 (4%)]\tLoss: 0.177134\n",
      "Train Epoch: 3 [7500/124800 (6%)]\tLoss: 0.298848\n",
      "Train Epoch: 3 [10000/124800 (8%)]\tLoss: 0.211677\n",
      "Train Epoch: 3 [12500/124800 (10%)]\tLoss: 0.273258\n",
      "Train Epoch: 3 [15000/124800 (12%)]\tLoss: 0.470691\n",
      "Train Epoch: 3 [17500/124800 (14%)]\tLoss: 0.390265\n",
      "Train Epoch: 3 [20000/124800 (16%)]\tLoss: 0.219174\n",
      "Train Epoch: 3 [22500/124800 (18%)]\tLoss: 0.220457\n",
      "Train Epoch: 3 [25000/124800 (20%)]\tLoss: 0.249383\n",
      "Train Epoch: 3 [27500/124800 (22%)]\tLoss: 0.162248\n",
      "Train Epoch: 3 [30000/124800 (24%)]\tLoss: 0.260360\n",
      "Train Epoch: 3 [32500/124800 (26%)]\tLoss: 0.268302\n",
      "Train Epoch: 3 [35000/124800 (28%)]\tLoss: 0.437775\n",
      "Train Epoch: 3 [37500/124800 (30%)]\tLoss: 0.378584\n",
      "Train Epoch: 3 [40000/124800 (32%)]\tLoss: 0.136401\n",
      "Train Epoch: 3 [42500/124800 (34%)]\tLoss: 0.324243\n",
      "Train Epoch: 3 [45000/124800 (36%)]\tLoss: 0.161765\n",
      "Train Epoch: 3 [47500/124800 (38%)]\tLoss: 0.268727\n",
      "Train Epoch: 3 [50000/124800 (40%)]\tLoss: 0.144779\n",
      "Train Epoch: 3 [52500/124800 (42%)]\tLoss: 0.416679\n",
      "Train Epoch: 3 [55000/124800 (44%)]\tLoss: 0.303940\n",
      "Train Epoch: 3 [57500/124800 (46%)]\tLoss: 0.438315\n",
      "Train Epoch: 3 [60000/124800 (48%)]\tLoss: 0.545112\n",
      "Train Epoch: 3 [62500/124800 (50%)]\tLoss: 0.279784\n",
      "Train Epoch: 3 [65000/124800 (52%)]\tLoss: 0.323075\n",
      "Train Epoch: 3 [67500/124800 (54%)]\tLoss: 0.398294\n",
      "Train Epoch: 3 [70000/124800 (56%)]\tLoss: 0.283064\n",
      "Train Epoch: 3 [72500/124800 (58%)]\tLoss: 0.365865\n",
      "Train Epoch: 3 [75000/124800 (60%)]\tLoss: 0.233798\n",
      "Train Epoch: 3 [77500/124800 (62%)]\tLoss: 0.591150\n",
      "Train Epoch: 3 [80000/124800 (64%)]\tLoss: 0.382018\n",
      "Train Epoch: 3 [82500/124800 (66%)]\tLoss: 0.171808\n",
      "Train Epoch: 3 [85000/124800 (68%)]\tLoss: 0.345596\n",
      "Train Epoch: 3 [87500/124800 (70%)]\tLoss: 0.152521\n",
      "Train Epoch: 3 [90000/124800 (72%)]\tLoss: 0.209399\n",
      "Train Epoch: 3 [92500/124800 (74%)]\tLoss: 0.273153\n",
      "Train Epoch: 3 [95000/124800 (76%)]\tLoss: 0.099037\n",
      "Train Epoch: 3 [97500/124800 (78%)]\tLoss: 0.178964\n",
      "Train Epoch: 3 [100000/124800 (80%)]\tLoss: 0.589300\n",
      "Train Epoch: 3 [102500/124800 (82%)]\tLoss: 0.273008\n",
      "Train Epoch: 3 [105000/124800 (84%)]\tLoss: 0.211900\n",
      "Train Epoch: 3 [107500/124800 (86%)]\tLoss: 0.343829\n",
      "Train Epoch: 3 [110000/124800 (88%)]\tLoss: 0.198330\n",
      "Train Epoch: 3 [112500/124800 (90%)]\tLoss: 0.278606\n",
      "Train Epoch: 3 [115000/124800 (92%)]\tLoss: 0.153291\n",
      "Train Epoch: 3 [117500/124800 (94%)]\tLoss: 0.252129\n",
      "Train Epoch: 3 [120000/124800 (96%)]\tLoss: 0.314463\n",
      "Train Epoch: 3 [122500/124800 (98%)]\tLoss: 0.098739\n",
      "\n",
      "Test set: Avg. Loss: 0.2630, Accuracy: 18980/20800 (91%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importing the training data from the training file already present in EMNIST dataset\n",
    "training = torch.utils.data.DataLoader(torchvision.datasets.EMNIST(root = '/results/', split = 'letters' , train = True, download = True, transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),torchvision.transforms.Normalize((0.1307,),(0.3081,))])),batch_size = 50, shuffle = True)\n",
    "    # importing the testing data\n",
    "testing = torch.utils.data.DataLoader(torchvision.datasets.EMNIST(root = '/results/', split = 'letters' , train = False, download = True, transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),torchvision.transforms.Normalize((0.1307,),(0.3081,))])),batch_size = 1000, shuffle = True)   \n",
    "    # creating the neural network class. this class will consist of 3 convolutional layers, 3 max pooling layers and 2 linear layers for now. these layers will be connected by the widely used ReLU activation function. we will be using the Log softmax function for the output layer. these parameters will be varied during the subsequent runs to improve the accuracy\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "    #         input is 1X28X28, output is 26 letters\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=4, stride=1, padding=2)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size = 4, stride = 1, padding = 2)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n",
    "        self.conv3 = nn.Conv2d(20, 30, kernel_size = 3, stride = 1, padding = 2)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n",
    "        self.fc1 = nn.Linear(30 * 4 * 4, 150)\n",
    "        self.fc2 = nn.Linear(150, 27)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = x.view(-1, 30 * 4 *4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "            # defining the relationship between input and output sizes for usage afterwards\n",
    "def outputsize(in_size,kernel_size,stride,padding):\n",
    "    output = int((in_size-kernel_size+2*(padding)) / stride) + 1\n",
    "    return output\n",
    "    # defining the various hyperparameters\n",
    "epochs = 3 \n",
    "log_interval = 50\n",
    "network = Net()\n",
    "optimizer = optim.SGD(network.parameters(), lr = 0.01, momentum = 0.21)\n",
    "    # creating counters for our results i.e for training losses, test losses, no. of sample trained and no.of samples tested\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(training.dataset) for i in range(epochs+1)]\n",
    "        # defining the training network\n",
    "def train(epoch):\n",
    "    network.train()\n",
    "    for batch_idx, (data,target) in enumerate(training):\n",
    "    #       making all the gradients zero before the start of each epoch\n",
    "        optimizer.zero_grad()\n",
    "    #     input output and backpropagation steps along with the application of NLL loss function for now\n",
    "        output = network(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    #         printing the results after each mini batch has been trained during a particular epoch\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx*len(data), len(training.dataset), 100. *batch_idx / len(training), loss.item()))\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append((batch_idx*100) + ((epoch-1)*len(training.dataset)))\n",
    "            torch.save(network.state_dict(), '/results/model.pth')\n",
    "            torch.save(optimizer.state_dict(), '/results/optimizer.pth')\n",
    "    # defining the testing network\n",
    "def test():\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    losstest = []\n",
    "    #   calculating the loss for each sample and the accuracy\n",
    "    with torch.no_grad():\n",
    "        for data, target in testing:\n",
    "            output = network(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average = False).item()\n",
    "            pred = output.data.max(1, keepdim = True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    #         initialising the various parameters to calculate accuracy and print it\n",
    "        test_loss /= len(testing.dataset)\n",
    "        losstest.append(test_loss)\n",
    "        print('\\nTest set: Avg. Loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(testing.dataset), 100. *correct / len(testing.dataset)))\n",
    "    # initializing the entire network with the datasets and using the above functions to calculate the accuracy\n",
    "    # first checking the accuracy with random guesses(network uninitialised)\n",
    "test()\n",
    "for epoch in range(1,epochs+1):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. Loss: 3.2934, Accuracy: 1003/20800 (4%)\n",
      "\n",
      "Train Epoch: 1 [0/124800 (0%)]\tLoss: 3.305189\n",
      "Train Epoch: 1 [2500/124800 (2%)]\tLoss: 3.288908\n",
      "Train Epoch: 1 [5000/124800 (4%)]\tLoss: 3.276422\n",
      "Train Epoch: 1 [7500/124800 (6%)]\tLoss: 3.279228\n",
      "Train Epoch: 1 [10000/124800 (8%)]\tLoss: 3.286280\n",
      "Train Epoch: 1 [12500/124800 (10%)]\tLoss: 3.280068\n",
      "Train Epoch: 1 [15000/124800 (12%)]\tLoss: 3.260219\n",
      "Train Epoch: 1 [17500/124800 (14%)]\tLoss: 3.232932\n",
      "Train Epoch: 1 [20000/124800 (16%)]\tLoss: 3.236946\n",
      "Train Epoch: 1 [22500/124800 (18%)]\tLoss: 3.142375\n",
      "Train Epoch: 1 [25000/124800 (20%)]\tLoss: 2.906212\n",
      "Train Epoch: 1 [27500/124800 (22%)]\tLoss: 2.329659\n",
      "Train Epoch: 1 [30000/124800 (24%)]\tLoss: 2.211605\n",
      "Train Epoch: 1 [32500/124800 (26%)]\tLoss: 1.483033\n",
      "Train Epoch: 1 [35000/124800 (28%)]\tLoss: 1.324959\n",
      "Train Epoch: 1 [37500/124800 (30%)]\tLoss: 1.471568\n",
      "Train Epoch: 1 [40000/124800 (32%)]\tLoss: 1.071289\n",
      "Train Epoch: 1 [42500/124800 (34%)]\tLoss: 1.388818\n",
      "Train Epoch: 1 [45000/124800 (36%)]\tLoss: 1.144225\n",
      "Train Epoch: 1 [47500/124800 (38%)]\tLoss: 1.043933\n",
      "Train Epoch: 1 [50000/124800 (40%)]\tLoss: 0.756821\n",
      "Train Epoch: 1 [52500/124800 (42%)]\tLoss: 0.629349\n",
      "Train Epoch: 1 [55000/124800 (44%)]\tLoss: 0.809299\n",
      "Train Epoch: 1 [57500/124800 (46%)]\tLoss: 0.881865\n",
      "Train Epoch: 1 [60000/124800 (48%)]\tLoss: 0.546128\n",
      "Train Epoch: 1 [62500/124800 (50%)]\tLoss: 0.643307\n",
      "Train Epoch: 1 [65000/124800 (52%)]\tLoss: 0.704325\n",
      "Train Epoch: 1 [67500/124800 (54%)]\tLoss: 0.379089\n",
      "Train Epoch: 1 [70000/124800 (56%)]\tLoss: 0.483528\n",
      "Train Epoch: 1 [72500/124800 (58%)]\tLoss: 0.875556\n",
      "Train Epoch: 1 [75000/124800 (60%)]\tLoss: 0.256098\n",
      "Train Epoch: 1 [77500/124800 (62%)]\tLoss: 0.603773\n",
      "Train Epoch: 1 [80000/124800 (64%)]\tLoss: 0.273485\n",
      "Train Epoch: 1 [82500/124800 (66%)]\tLoss: 0.628209\n",
      "Train Epoch: 1 [85000/124800 (68%)]\tLoss: 0.709959\n",
      "Train Epoch: 1 [87500/124800 (70%)]\tLoss: 0.637948\n",
      "Train Epoch: 1 [90000/124800 (72%)]\tLoss: 0.660788\n",
      "Train Epoch: 1 [92500/124800 (74%)]\tLoss: 0.405947\n",
      "Train Epoch: 1 [95000/124800 (76%)]\tLoss: 0.378237\n",
      "Train Epoch: 1 [97500/124800 (78%)]\tLoss: 0.425984\n",
      "Train Epoch: 1 [100000/124800 (80%)]\tLoss: 0.525367\n",
      "Train Epoch: 1 [102500/124800 (82%)]\tLoss: 0.434311\n",
      "Train Epoch: 1 [105000/124800 (84%)]\tLoss: 0.251779\n",
      "Train Epoch: 1 [107500/124800 (86%)]\tLoss: 0.666647\n",
      "Train Epoch: 1 [110000/124800 (88%)]\tLoss: 0.424790\n",
      "Train Epoch: 1 [112500/124800 (90%)]\tLoss: 0.436737\n",
      "Train Epoch: 1 [115000/124800 (92%)]\tLoss: 0.536456\n",
      "Train Epoch: 1 [117500/124800 (94%)]\tLoss: 0.276993\n",
      "Train Epoch: 1 [120000/124800 (96%)]\tLoss: 0.437644\n",
      "Train Epoch: 1 [122500/124800 (98%)]\tLoss: 0.586484\n",
      "\n",
      "Test set: Avg. Loss: 0.4542, Accuracy: 17720/20800 (85%)\n",
      "\n",
      "Train Epoch: 2 [0/124800 (0%)]\tLoss: 0.425437\n",
      "Train Epoch: 2 [2500/124800 (2%)]\tLoss: 0.360800\n",
      "Train Epoch: 2 [5000/124800 (4%)]\tLoss: 0.521655\n",
      "Train Epoch: 2 [7500/124800 (6%)]\tLoss: 0.388687\n",
      "Train Epoch: 2 [10000/124800 (8%)]\tLoss: 0.562196\n",
      "Train Epoch: 2 [12500/124800 (10%)]\tLoss: 0.655941\n",
      "Train Epoch: 2 [15000/124800 (12%)]\tLoss: 0.323946\n",
      "Train Epoch: 2 [17500/124800 (14%)]\tLoss: 0.330353\n",
      "Train Epoch: 2 [20000/124800 (16%)]\tLoss: 0.510516\n",
      "Train Epoch: 2 [22500/124800 (18%)]\tLoss: 0.472433\n",
      "Train Epoch: 2 [25000/124800 (20%)]\tLoss: 0.155048\n",
      "Train Epoch: 2 [27500/124800 (22%)]\tLoss: 0.389706\n",
      "Train Epoch: 2 [30000/124800 (24%)]\tLoss: 0.475989\n",
      "Train Epoch: 2 [32500/124800 (26%)]\tLoss: 0.345322\n",
      "Train Epoch: 2 [35000/124800 (28%)]\tLoss: 0.708233\n",
      "Train Epoch: 2 [37500/124800 (30%)]\tLoss: 0.498271\n",
      "Train Epoch: 2 [40000/124800 (32%)]\tLoss: 0.533054\n",
      "Train Epoch: 2 [42500/124800 (34%)]\tLoss: 0.295860\n",
      "Train Epoch: 2 [45000/124800 (36%)]\tLoss: 0.648583\n",
      "Train Epoch: 2 [47500/124800 (38%)]\tLoss: 0.395362\n",
      "Train Epoch: 2 [50000/124800 (40%)]\tLoss: 0.276018\n",
      "Train Epoch: 2 [52500/124800 (42%)]\tLoss: 0.312173\n",
      "Train Epoch: 2 [55000/124800 (44%)]\tLoss: 0.369737\n",
      "Train Epoch: 2 [57500/124800 (46%)]\tLoss: 0.288049\n",
      "Train Epoch: 2 [60000/124800 (48%)]\tLoss: 0.260048\n",
      "Train Epoch: 2 [62500/124800 (50%)]\tLoss: 0.340366\n",
      "Train Epoch: 2 [65000/124800 (52%)]\tLoss: 0.322519\n",
      "Train Epoch: 2 [67500/124800 (54%)]\tLoss: 0.480797\n",
      "Train Epoch: 2 [70000/124800 (56%)]\tLoss: 0.403150\n",
      "Train Epoch: 2 [72500/124800 (58%)]\tLoss: 0.282910\n",
      "Train Epoch: 2 [75000/124800 (60%)]\tLoss: 0.239709\n",
      "Train Epoch: 2 [77500/124800 (62%)]\tLoss: 0.355157\n",
      "Train Epoch: 2 [80000/124800 (64%)]\tLoss: 0.518528\n",
      "Train Epoch: 2 [82500/124800 (66%)]\tLoss: 0.272150\n",
      "Train Epoch: 2 [85000/124800 (68%)]\tLoss: 0.377291\n",
      "Train Epoch: 2 [87500/124800 (70%)]\tLoss: 0.212992\n",
      "Train Epoch: 2 [90000/124800 (72%)]\tLoss: 0.352332\n",
      "Train Epoch: 2 [92500/124800 (74%)]\tLoss: 0.196680\n",
      "Train Epoch: 2 [95000/124800 (76%)]\tLoss: 0.375050\n",
      "Train Epoch: 2 [97500/124800 (78%)]\tLoss: 0.215475\n",
      "Train Epoch: 2 [100000/124800 (80%)]\tLoss: 0.402095\n",
      "Train Epoch: 2 [102500/124800 (82%)]\tLoss: 0.225676\n",
      "Train Epoch: 2 [105000/124800 (84%)]\tLoss: 0.283598\n",
      "Train Epoch: 2 [107500/124800 (86%)]\tLoss: 0.274717\n",
      "Train Epoch: 2 [110000/124800 (88%)]\tLoss: 0.193631\n",
      "Train Epoch: 2 [112500/124800 (90%)]\tLoss: 0.302582\n",
      "Train Epoch: 2 [115000/124800 (92%)]\tLoss: 0.471315\n",
      "Train Epoch: 2 [117500/124800 (94%)]\tLoss: 0.357070\n",
      "Train Epoch: 2 [120000/124800 (96%)]\tLoss: 0.573646\n",
      "Train Epoch: 2 [122500/124800 (98%)]\tLoss: 0.519149\n",
      "\n",
      "Test set: Avg. Loss: 0.3358, Accuracy: 18428/20800 (88%)\n",
      "\n",
      "Train Epoch: 3 [0/124800 (0%)]\tLoss: 0.401923\n",
      "Train Epoch: 3 [2500/124800 (2%)]\tLoss: 0.089445\n",
      "Train Epoch: 3 [5000/124800 (4%)]\tLoss: 0.208938\n",
      "Train Epoch: 3 [7500/124800 (6%)]\tLoss: 0.383977\n",
      "Train Epoch: 3 [10000/124800 (8%)]\tLoss: 0.258793\n",
      "Train Epoch: 3 [12500/124800 (10%)]\tLoss: 0.363432\n",
      "Train Epoch: 3 [15000/124800 (12%)]\tLoss: 0.417600\n",
      "Train Epoch: 3 [17500/124800 (14%)]\tLoss: 0.199851\n",
      "Train Epoch: 3 [20000/124800 (16%)]\tLoss: 0.307427\n",
      "Train Epoch: 3 [22500/124800 (18%)]\tLoss: 0.292735\n",
      "Train Epoch: 3 [25000/124800 (20%)]\tLoss: 0.389058\n",
      "Train Epoch: 3 [27500/124800 (22%)]\tLoss: 0.293660\n",
      "Train Epoch: 3 [30000/124800 (24%)]\tLoss: 0.265540\n",
      "Train Epoch: 3 [32500/124800 (26%)]\tLoss: 0.331973\n",
      "Train Epoch: 3 [35000/124800 (28%)]\tLoss: 0.372992\n",
      "Train Epoch: 3 [37500/124800 (30%)]\tLoss: 0.196856\n",
      "Train Epoch: 3 [40000/124800 (32%)]\tLoss: 0.793054\n",
      "Train Epoch: 3 [42500/124800 (34%)]\tLoss: 0.208397\n",
      "Train Epoch: 3 [45000/124800 (36%)]\tLoss: 0.186993\n",
      "Train Epoch: 3 [47500/124800 (38%)]\tLoss: 0.469909\n",
      "Train Epoch: 3 [50000/124800 (40%)]\tLoss: 0.691462\n",
      "Train Epoch: 3 [52500/124800 (42%)]\tLoss: 0.212245\n",
      "Train Epoch: 3 [55000/124800 (44%)]\tLoss: 0.175122\n",
      "Train Epoch: 3 [57500/124800 (46%)]\tLoss: 0.316905\n",
      "Train Epoch: 3 [60000/124800 (48%)]\tLoss: 0.419698\n",
      "Train Epoch: 3 [62500/124800 (50%)]\tLoss: 0.238255\n",
      "Train Epoch: 3 [65000/124800 (52%)]\tLoss: 0.668560\n",
      "Train Epoch: 3 [67500/124800 (54%)]\tLoss: 0.241493\n",
      "Train Epoch: 3 [70000/124800 (56%)]\tLoss: 0.391240\n",
      "Train Epoch: 3 [72500/124800 (58%)]\tLoss: 0.369191\n",
      "Train Epoch: 3 [75000/124800 (60%)]\tLoss: 0.238424\n",
      "Train Epoch: 3 [77500/124800 (62%)]\tLoss: 0.114979\n",
      "Train Epoch: 3 [80000/124800 (64%)]\tLoss: 0.391654\n",
      "Train Epoch: 3 [82500/124800 (66%)]\tLoss: 0.109878\n",
      "Train Epoch: 3 [85000/124800 (68%)]\tLoss: 0.209513\n",
      "Train Epoch: 3 [87500/124800 (70%)]\tLoss: 0.479896\n",
      "Train Epoch: 3 [90000/124800 (72%)]\tLoss: 0.183723\n",
      "Train Epoch: 3 [92500/124800 (74%)]\tLoss: 0.247552\n",
      "Train Epoch: 3 [95000/124800 (76%)]\tLoss: 0.355478\n",
      "Train Epoch: 3 [97500/124800 (78%)]\tLoss: 0.624233\n",
      "Train Epoch: 3 [100000/124800 (80%)]\tLoss: 0.371181\n",
      "Train Epoch: 3 [102500/124800 (82%)]\tLoss: 0.151665\n",
      "Train Epoch: 3 [105000/124800 (84%)]\tLoss: 0.523657\n",
      "Train Epoch: 3 [107500/124800 (86%)]\tLoss: 0.468407\n",
      "Train Epoch: 3 [110000/124800 (88%)]\tLoss: 0.201564\n",
      "Train Epoch: 3 [112500/124800 (90%)]\tLoss: 0.298442\n",
      "Train Epoch: 3 [115000/124800 (92%)]\tLoss: 0.272899\n",
      "Train Epoch: 3 [117500/124800 (94%)]\tLoss: 0.405017\n",
      "Train Epoch: 3 [120000/124800 (96%)]\tLoss: 0.323415\n",
      "Train Epoch: 3 [122500/124800 (98%)]\tLoss: 0.319106\n",
      "\n",
      "Test set: Avg. Loss: 0.3003, Accuracy: 18791/20800 (90%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importing the training data from the training file already present in EMNIST dataset\n",
    "training = torch.utils.data.DataLoader(torchvision.datasets.EMNIST(root = '/results/', split = 'letters' , train = True, download = True, transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),torchvision.transforms.Normalize((0.1307,),(0.3081,))])),batch_size = 50, shuffle = True)\n",
    "    # importing the testing data\n",
    "testing = torch.utils.data.DataLoader(torchvision.datasets.EMNIST(root = '/results/', split = 'letters' , train = False, download = True, transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),torchvision.transforms.Normalize((0.1307,),(0.3081,))])),batch_size = 1000, shuffle = True)   \n",
    "    # creating the neural network class. this class will consist of 3 convolutional layers, 3 max pooling layers and 2 linear layers for now. these layers will be connected by the widely used ReLU activation function. we will be using the Log softmax function for the output layer. these parameters will be varied during the subsequent runs to improve the accuracy\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "    #         input is 1X28X28, output is 26 letters\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=4, stride=1, padding=2)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size = 3, stride = 1, padding = 2)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n",
    "        self.conv3 = nn.Conv2d(20, 30, kernel_size = 4, stride = 1, padding = 2)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n",
    "        self.fc1 = nn.Linear(30 * 4 * 4, 150)\n",
    "        self.fc2 = nn.Linear(150, 27)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = x.view(-1, 30 * 4 *4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "            # defining the relationship between input and output sizes for usage afterwards\n",
    "def outputsize(in_size,kernel_size,stride,padding):\n",
    "    output = int((in_size-kernel_size+2*(padding)) / stride) + 1\n",
    "    return output\n",
    "    # defining the various hyperparameters\n",
    "epochs = 3 \n",
    "log_interval = 50\n",
    "network = Net()\n",
    "optimizer = optim.SGD(network.parameters(), lr = 0.01, momentum = 0.21)\n",
    "    # creating counters for our results i.e for training losses, test losses, no. of sample trained and no.of samples tested\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(training.dataset) for i in range(epochs+1)]\n",
    "        # defining the training network\n",
    "def train(epoch):\n",
    "    network.train()\n",
    "    for batch_idx, (data,target) in enumerate(training):\n",
    "    #       making all the gradients zero before the start of each epoch\n",
    "        optimizer.zero_grad()\n",
    "    #     input output and backpropagation steps along with the application of NLL loss function for now\n",
    "        output = network(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    #         printing the results after each mini batch has been trained during a particular epoch\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx*len(data), len(training.dataset), 100. *batch_idx / len(training), loss.item()))\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append((batch_idx*100) + ((epoch-1)*len(training.dataset)))\n",
    "            torch.save(network.state_dict(), '/results/model.pth')\n",
    "            torch.save(optimizer.state_dict(), '/results/optimizer.pth')\n",
    "    # defining the testing network\n",
    "def test():\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    losstest = []\n",
    "    #   calculating the loss for each sample and the accuracy\n",
    "    with torch.no_grad():\n",
    "        for data, target in testing:\n",
    "            output = network(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average = False).item()\n",
    "            pred = output.data.max(1, keepdim = True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    #         initialising the various parameters to calculate accuracy and print it\n",
    "        test_loss /= len(testing.dataset)\n",
    "        losstest.append(test_loss)\n",
    "        print('\\nTest set: Avg. Loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(testing.dataset), 100. *correct / len(testing.dataset)))\n",
    "    # initializing the entire network with the datasets and using the above functions to calculate the accuracy\n",
    "    # first checking the accuracy with random guesses(network uninitialised)\n",
    "test()\n",
    "for epoch in range(1,epochs+1):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
